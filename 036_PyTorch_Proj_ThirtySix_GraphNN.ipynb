{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PKOjRlDroLc9",
        "02_95RZ0ouer",
        "ZDzlzOGHpENd",
        "rt8Lm2AUvBZy",
        "L1BGewe4u5TJ"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP0McVDRAONYedBi5G2Aw+u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariahosseini/DeepML/blob/main/036_PyTorch_Proj_ThirtySix_GraphNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-geometric"
      ],
      "metadata": {
        "id": "gJGgsbuUuLTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUTYzwsOoEM9"
      },
      "outputs": [],
      "source": [
        "# utils\n",
        "import os\n",
        "import collections\n",
        "import time\n",
        "import scipy\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import scipy.sparse, scipy.sparse.linalg\n",
        "from numpy import linalg as LA\n",
        "from functools import partial\n",
        "# sklearn\n",
        "import sklearn\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.datasets import KarateClub\n",
        "from torch_geometric.utils import to_networkx\n",
        "# vis\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print('cuda available')\n",
        "    dtypeFloat = torch.cuda.FloatTensor\n",
        "    dtypeLong = torch.cuda.LongTensor\n",
        "    torch.cuda.manual_seed(1)\n",
        "else:\n",
        "    print('cuda not available')\n",
        "    dtypeFloat = torch.FloatTensor\n",
        "    dtypeLong = torch.LongTensor\n",
        "    torch.manual_seed(1)"
      ],
      "metadata": {
        "id": "3QWvEEU6xRey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "wfKUVmBXt9fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coarsening\n",
        "\n"
      ],
      "metadata": {
        "id": "PKOjRlDroLc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# graph coarsening with Heavy Edge Matching\n",
        "def coarsen(A, levels,laplacian_func):\n",
        "\n",
        "    graphs, parents = HEM(A, levels)\n",
        "    perms = compute_perm(parents)\n",
        "\n",
        "    laplacians = []\n",
        "    for i,A in enumerate(graphs):\n",
        "        M, M = A.shape\n",
        "\n",
        "        if i < levels:\n",
        "            A = perm_adjacency(A, perms[i])\n",
        "\n",
        "        A = A.tocsr()\n",
        "        A.eliminate_zeros()\n",
        "        Mnew, Mnew = A.shape\n",
        "        print('Layer {0}: M_{0} = |V| = {1} nodes ({2} added), |E| = {3} edges'.format(i, Mnew, Mnew-M, A.nnz//2))\n",
        "\n",
        "        L = laplacian_func(A)\n",
        "        laplacians.append(L)\n",
        "\n",
        "    return laplacians, perms[0] if len(perms) > 0 else None\n",
        "\n",
        "\n",
        "def HEM(W, levels, rid=None):\n",
        "    \"\"\"\n",
        "    Coarsen a graph multiple times using the Heavy Edge Matching (HEM).\n",
        "\n",
        "    Input\n",
        "    W: symmetric sparse weight (adjacency) matrix\n",
        "    levels: the number of coarsened graphs\n",
        "\n",
        "    Output\n",
        "    graph[0]: original graph of size N_1\n",
        "    graph[2]: coarser graph of size N_2 < N_1\n",
        "    graph[levels]: coarsest graph of Size N_levels < ... < N_2 < N_1\n",
        "    parents[i] is a vector of size N_i with entries ranging from 1 to N_{i+1}\n",
        "        which indicate the parents in the coarser graph[i+1]\n",
        "    nd_sz{i} is a vector of size N_i that contains the size of the supernode in the graph{i}\n",
        "\n",
        "    Note\n",
        "    if \"graph\" is a list of length k, then \"parents\" will be a list of length k-1\n",
        "    \"\"\"\n",
        "\n",
        "    N, N = W.shape\n",
        "\n",
        "    if rid is None:\n",
        "        rid = np.random.permutation(range(N))\n",
        "\n",
        "    ss = np.array(W.sum(axis=0)).squeeze()\n",
        "    rid = np.argsort(ss)\n",
        "\n",
        "\n",
        "    parents = []\n",
        "    degree = W.sum(axis=0) - W.diagonal()\n",
        "    graphs = []\n",
        "    graphs.append(W)\n",
        "\n",
        "    print('Heavy Edge Matching coarsening with Xavier version')\n",
        "\n",
        "    for _ in range(levels):\n",
        "\n",
        "        # CHOOSE THE WEIGHTS FOR THE PAIRING\n",
        "        # weights = ones(N,1)       # metis weights\n",
        "        weights = degree            # graclus weights\n",
        "        # weights = supernode_size  # other possibility\n",
        "        weights = np.array(weights).squeeze()\n",
        "\n",
        "        # PAIR THE VERTICES AND CONSTRUCT THE ROOT VECTOR\n",
        "        idx_row, idx_col, val = scipy.sparse.find(W)\n",
        "        cc = idx_row\n",
        "        rr = idx_col\n",
        "        vv = val\n",
        "\n",
        "        # TO BE SPEEDUP\n",
        "        if not (list(cc)==list(np.sort(cc))):\n",
        "            tmp=cc\n",
        "            cc=rr\n",
        "            rr=tmp\n",
        "\n",
        "        cluster_id = HEM_one_level(cc,rr,vv,rid,weights) # cc is ordered\n",
        "        parents.append(cluster_id)\n",
        "\n",
        "        # COMPUTE THE EDGES WEIGHTS FOR THE NEW GRAPH\n",
        "        nrr = cluster_id[rr]\n",
        "        ncc = cluster_id[cc]\n",
        "        nvv = vv\n",
        "        Nnew = cluster_id.max() + 1\n",
        "        # CSR is more appropriate: row,val pairs appear multiple times\n",
        "        W = scipy.sparse.csr_matrix((nvv,(nrr,ncc)), shape=(Nnew,Nnew))\n",
        "        W.eliminate_zeros()\n",
        "\n",
        "        # add new graph to the list of all coarsened graphs\n",
        "        graphs.append(W)\n",
        "        N, N = W.shape\n",
        "\n",
        "        # COMPUTE THE DEGREE (OMIT OR NOT SELF LOOPS)\n",
        "        degree = W.sum(axis=0)\n",
        "        # degree = W.sum(axis=0) - W.diagonal()\n",
        "\n",
        "        # CHOOSE THE ORDER IN WHICH VERTICES WILL BE VISTED AT THE NEXT PASS\n",
        "        # [~, rid]=sort(ss);     # arthur strategy\n",
        "        # [~, rid]=sort(supernode_size);    #  thomas strategy\n",
        "        # rid=randperm(N);                  #  metis/graclus strategy\n",
        "        ss = np.array(W.sum(axis=0)).squeeze()\n",
        "        rid = np.argsort(ss)\n",
        "\n",
        "    return graphs, parents\n",
        "\n",
        "\n",
        "# coarsen a graph given by rr,cc,vv.  rr is assumed to be ordered\n",
        "def HEM_one_level(rr,cc,vv,rid,weights):\n",
        "\n",
        "    nnz = rr.shape[0]\n",
        "    N = rr[nnz-1] + 1\n",
        "\n",
        "    marked = np.zeros(N, np.bool)\n",
        "    rowstart = np.zeros(N, np.int32)\n",
        "    rowlength = np.zeros(N, np.int32)\n",
        "    cluster_id = np.zeros(N, np.int32)\n",
        "\n",
        "    oldval = rr[0]\n",
        "    count = 0\n",
        "    clustercount = 0\n",
        "\n",
        "    for ii in range(nnz):\n",
        "        rowlength[count] = rowlength[count] + 1\n",
        "        if rr[ii] > oldval:\n",
        "            oldval = rr[ii]\n",
        "            rowstart[count+1] = ii\n",
        "            count = count + 1\n",
        "\n",
        "    for ii in range(N):\n",
        "        tid = rid[ii]\n",
        "        if not marked[tid]:\n",
        "            wmax = 0.0\n",
        "            rs = rowstart[tid]\n",
        "            marked[tid] = True\n",
        "            bestneighbor = -1\n",
        "            for jj in range(rowlength[tid]):\n",
        "                nid = cc[rs+jj]\n",
        "                if marked[nid]:\n",
        "                    tval = 0.0\n",
        "                else:\n",
        "                    # first approach\n",
        "                    if 2==1:\n",
        "                        tval = vv[rs+jj] * (1.0/weights[tid] + 1.0/weights[nid])\n",
        "                    # second approach\n",
        "                    if 1==1:\n",
        "                        Wij = vv[rs+jj]\n",
        "                        Wii = vv[rowstart[tid]]\n",
        "                        Wjj = vv[rowstart[nid]]\n",
        "                        di = weights[tid]\n",
        "                        dj = weights[nid]\n",
        "                        tval = (2.*Wij + Wii + Wjj) * 1./(di+dj+1e-9)\n",
        "\n",
        "                if tval > wmax:\n",
        "                    wmax = tval\n",
        "                    bestneighbor = nid\n",
        "\n",
        "            cluster_id[tid] = clustercount\n",
        "\n",
        "            if bestneighbor > -1:\n",
        "                cluster_id[bestneighbor] = clustercount\n",
        "                marked[bestneighbor] = True\n",
        "\n",
        "            clustercount += 1\n",
        "\n",
        "    return cluster_id\n",
        "\n",
        "\n",
        "def compute_perm(parents):\n",
        "    \"\"\"\n",
        "    Return a list of indices to reorder the adjacency and data matrices so\n",
        "    that the union of two neighbors from layer to layer forms a binary tree.\n",
        "    \"\"\"\n",
        "\n",
        "    # order of last layer is random (chosen by the clustering algorithm).\n",
        "    indices = []\n",
        "    if len(parents) > 0:\n",
        "        M_last = max(parents[-1]) + 1\n",
        "        indices.append(list(range(M_last)))\n",
        "\n",
        "    for parent in parents[::-1]:\n",
        "\n",
        "        # fake nodes go after real ones.\n",
        "        pool_singeltons = len(parent)\n",
        "\n",
        "        indices_layer = []\n",
        "        for i in indices[-1]:\n",
        "            indices_node = list(np.where(parent == i)[0])\n",
        "            assert 0 <= len(indices_node) <= 2\n",
        "\n",
        "            # add a node to go with a singelton.\n",
        "            if len(indices_node) is 1:\n",
        "                indices_node.append(pool_singeltons)\n",
        "                pool_singeltons += 1\n",
        "\n",
        "            # add two nodes as children of a singelton in the parent.\n",
        "            elif len(indices_node) is 0:\n",
        "                indices_node.append(pool_singeltons+0)\n",
        "                indices_node.append(pool_singeltons+1)\n",
        "                pool_singeltons += 2\n",
        "\n",
        "            indices_layer.extend(indices_node)\n",
        "        indices.append(indices_layer)\n",
        "\n",
        "    # sanity checks.\n",
        "    for i,indices_layer in enumerate(indices):\n",
        "        M = M_last*2**i\n",
        "        # reduction by 2 at each layer (binary tree).\n",
        "        assert len(indices[0] == M)\n",
        "        # the new ordering does not omit an indice.\n",
        "        assert sorted(indices_layer) == list(range(M))\n",
        "\n",
        "    return indices[::-1]\n",
        "\n",
        "assert (compute_perm([np.array([4,1,1,2,2,3,0,0,3]),np.array([2,1,0,1,0])])\n",
        "        == [[3,4,0,9,1,2,5,8,6,7,10,11],[2,4,1,3,0,5],[0,1,2]])\n",
        "\n",
        "\n",
        "\n",
        "def perm_adjacency(A, indices):\n",
        "    \"\"\"\n",
        "    Permute adjacency matrix, i.e. exchange node ids,\n",
        "    so that binary unions form the clustering tree.\n",
        "    \"\"\"\n",
        "    if indices is None:\n",
        "        return A\n",
        "\n",
        "    M, M = A.shape\n",
        "    Mnew = len(indices)\n",
        "    A = A.tocoo()\n",
        "\n",
        "    # add Mnew - M isolated vertices.\n",
        "    rows = scipy.sparse.coo_matrix((Mnew-M,    M), dtype=np.float32)\n",
        "    cols = scipy.sparse.coo_matrix((Mnew, Mnew-M), dtype=np.float32)\n",
        "    A = scipy.sparse.vstack([A, rows])\n",
        "    A = scipy.sparse.hstack([A, cols])\n",
        "\n",
        "    # permute the rows and the columns.\n",
        "    perm = np.argsort(indices)\n",
        "    A.row = np.array(perm)[A.row]\n",
        "    A.col = np.array(perm)[A.col]\n",
        "\n",
        "    assert np.abs(A - A.T).mean() < 1e-8 # 1e-9\n",
        "    assert type(A) is scipy.sparse.coo.coo_matrix\n",
        "    return A\n",
        "\n",
        "\n",
        "\n",
        "def perm_data(x, indices):\n",
        "    \"\"\"\n",
        "    Permute data matrix, i.e. exchange node ids,\n",
        "    so that binary unions form the clustering tree.\n",
        "    \"\"\"\n",
        "    if indices is None:\n",
        "        return x\n",
        "\n",
        "    N, M = x.shape\n",
        "    Mnew = len(indices)\n",
        "    assert Mnew >= M\n",
        "    xnew = np.empty((N, Mnew))\n",
        "    for i,j in enumerate(indices):\n",
        "        # existing vertex, i.e. real data.\n",
        "        if j < M:\n",
        "            xnew[:,i] = x[:,j]\n",
        "        # fake vertex because of singeltons.\n",
        "        # they will stay 0 so that max pooling chooses the singelton\n",
        "        else:\n",
        "            xnew[:,i] = np.zeros(N)\n",
        "    return xnew"
      ],
      "metadata": {
        "id": "3w1wgnAPoOv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GridGraph"
      ],
      "metadata": {
        "id": "02_95RZ0ouer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_graph(grid_side,number_edges,metric):\n",
        "    \"\"\"Generate graph of a grid\"\"\"\n",
        "    z = grid(grid_side)\n",
        "    dist, idx = distance_sklearn_metrics(z, k=number_edges, metric=metric)\n",
        "    A = adjacency(dist, idx)\n",
        "    print(\"nb edges: \",A.nnz)\n",
        "    return A\n",
        "\n",
        "\n",
        "def grid(m, dtype=np.float32):\n",
        "    \"\"\"Return coordinates of grid points\"\"\"\n",
        "    M = m**2\n",
        "    x = np.linspace(0,1,m, dtype=dtype)\n",
        "    y = np.linspace(0,1,m, dtype=dtype)\n",
        "    xx, yy = np.meshgrid(x, y)\n",
        "    z = np.empty((M,2), dtype)\n",
        "    z[:,0] = xx.reshape(M)\n",
        "    z[:,1] = yy.reshape(M)\n",
        "    return z\n",
        "\n",
        "\n",
        "def distance_sklearn_metrics(z, k=4, metric='euclidean'):\n",
        "    \"\"\"Compute pairwise distances\"\"\"\n",
        "    # d = sklearn.metrics.pairwise.pairwise_distances(z, metric=metric, n_jobs=-2)\n",
        "    d = sklearn.metrics.pairwise.pairwise_distances(z, metric=metric, n_jobs=1)\n",
        "    # k-NN\n",
        "    idx = np.argsort(d)[:,1:k+1]\n",
        "    d.sort()\n",
        "    d = d[:,1:k+1]\n",
        "    return d, idx\n",
        "\n",
        "\n",
        "def adjacency(dist, idx):\n",
        "    \"\"\"Return adjacency matrix of a kNN graph\"\"\"\n",
        "    M, k = dist.shape\n",
        "    assert M, k == idx.shape\n",
        "    assert dist.min() >= 0\n",
        "    assert dist.max() <= 1\n",
        "\n",
        "    # pairwise distances\n",
        "    sigma2 = np.mean(dist[:,-1])**2\n",
        "    dist = np.exp(- dist**2 / sigma2)\n",
        "\n",
        "    # weight matrix\n",
        "    I = np.arange(0, M).repeat(k)\n",
        "    J = idx.reshape(M*k)\n",
        "    V = dist.reshape(M*k)\n",
        "    W = scipy.sparse.coo_matrix((V, (I, J)), shape=(M, M))\n",
        "\n",
        "    # no self-connections\n",
        "    W.setdiag(0)\n",
        "\n",
        "    # undirected graph\n",
        "    bigger = W.T > W\n",
        "    W = W - W.multiply(bigger) + W.T.multiply(bigger)\n",
        "\n",
        "    assert W.nnz % 2 == 0\n",
        "    assert np.abs(W - W.T).mean() < 1e-10\n",
        "    assert type(W) is scipy.sparse.csr.csr_matrix\n",
        "    return W"
      ],
      "metadata": {
        "id": "B9KVmo-hoxJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inductive Bias in GCN: A Spectral Perspective"
      ],
      "metadata": {
        "id": "ZDzlzOGHpENd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(h, color, cmap=\"Set1\"):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    if torch.is_tensor(h):\n",
        "        h = h.detach().cpu().numpy()\n",
        "        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=cmap)\n",
        "        [m0,m1] = np.median(h,axis=0)\n",
        "        [min0, min1] = np.min(h,axis=0)\n",
        "        [max0, max1] = np.max(h,axis=0)\n",
        "        plt.vlines(m0,min1,max1)\n",
        "        plt.hlines(m1,min0,max0)\n",
        "        for i in range(h.shape[0]):\n",
        "            plt.text(h[i,0], h[i,1], str(i))\n",
        "\n",
        "\n",
        "    else:\n",
        "        nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=True,\n",
        "                         node_color=color, cmap=cmap)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lVJvddzwpIsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = KarateClub()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')"
      ],
      "metadata": {
        "id": "iigrJX_GpIpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[0]\n",
        "biclasses = [int(b) for b in ((data.y == data.y[0]) + (data.y==data.y[5]))]"
      ],
      "metadata": {
        "id": "WGtnn8nnuIRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = to_networkx(data, to_undirected=True)\n",
        "visualize(G, color=biclasses)"
      ],
      "metadata": {
        "id": "fOhNZQt2pIxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def acc(predictions, classes):\n",
        "    n_tot = len(classes)\n",
        "    acc = np.sum([int(pred)==cla for pred,cla in zip(predictions,classes)])\n",
        "    return max(acc, n_tot-acc), n_tot"
      ],
      "metadata": {
        "id": "naTtuUeOpI2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c1, c2 = nx.algorithms.community.kernighan_lin_bisection(G)\n",
        "classes_kl = [0 if i in c1 else 1 for i in range(34)]\n",
        "visualize(G, color=classes_kl, cmap=\"Set2\")"
      ],
      "metadata": {
        "id": "NE1QOyEJpI4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc(classes_kl, biclasses)"
      ],
      "metadata": {
        "id": "QcM9TQ1spI80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_simu = 1000\n",
        "all_acc = np.zeros(n_simu)\n",
        "for i in range(n_simu):\n",
        "    c1,c2 = nx.algorithms.community.kernighan_lin_bisection(G)\n",
        "    classes_kl = [0 if i in c1 else 1 for i in range(34)]\n",
        "    all_acc[i],_ = acc(classes_kl, biclasses)"
      ],
      "metadata": {
        "id": "yM3B3ryupI-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bin_list = range(17,35)\n",
        "_ = plt.hist(all_acc, bins=bin_list,rwidth=0.8)"
      ],
      "metadata": {
        "id": "NUtmAPkTpJCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inductive bias for GCN"
      ],
      "metadata": {
        "id": "rt8Lm2AUvBZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(data.num_nodes, 4)# no feature...\n",
        "        self.conv2 = GCNConv(4, 4)\n",
        "        self.conv3 = GCNConv(4, 2)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        h = self.conv1(x, edge_index)\n",
        "        h = h.tanh()\n",
        "        h = self.conv2(h, edge_index)\n",
        "        h = h.tanh()\n",
        "        h = self.conv3(h, edge_index)\n",
        "        return h\n",
        "\n",
        "torch.manual_seed(12345)\n",
        "model = GCN()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "qMvC-W84pJFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = model(data.x, data.edge_index)\n",
        "visualize(h, color=biclasses)"
      ],
      "metadata": {
        "id": "xZjyeeYRpJKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def color_from_vec(vec,m=None):\n",
        "    if torch.is_tensor(vec):\n",
        "        vec = vec.detach().cpu().numpy()\n",
        "    if not(m):\n",
        "        m = np.median(vec,axis=0)\n",
        "    return np.array(vec < m)"
      ],
      "metadata": {
        "id": "sYJB_YM7pJRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "color_out = color_from_vec(h[:,0])\n",
        "visualize(G, color=color_out, cmap=\"Set2\")"
      ],
      "metadata": {
        "id": "dWA2WNyNuved"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc(color_out,biclasses)"
      ],
      "metadata": {
        "id": "Etp5dSV-uvh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_acc = np.zeros(n_simu)\n",
        "for i in range(n_simu):\n",
        "    model = GCN()\n",
        "    h = model(data.x, data.edge_index)\n",
        "    color_out = color_from_vec(h[:,0])\n",
        "    all_acc[i],_ = acc(color_out,biclasses)"
      ],
      "metadata": {
        "id": "qpNI08g6uvlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = plt.hist(all_acc, bins=bin_list,rwidth=0.8)"
      ],
      "metadata": {
        "id": "dAphxtjXuvoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(all_acc)"
      ],
      "metadata": {
        "id": "Fo6_zH3Suvsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spectral analysis of GCN"
      ],
      "metadata": {
        "id": "L1BGewe4u5TJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = nx.adjacency_matrix(G).todense()\n",
        "A_l = A + np.eye(A.shape[0],dtype=int)\n",
        "deg_l = np.dot(A_l,np.ones(A.shape[0]))\n",
        "scaling = np.dot(np.transpose(1/np.sqrt(deg_l)),(1/np.sqrt(deg_l)))\n",
        "S = np.multiply(scaling,A_l)\n",
        "eigen_values, eigen_vectors = LA.eigh(S)\n",
        "\n",
        "_ = plt.hist(eigen_values, bins = 40)"
      ],
      "metadata": {
        "id": "0jfUbkZzu4is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fiedler = np.array(eigen_vectors[:,-2]).squeeze()\n",
        "H1 = G.subgraph([i for (i,f) in enumerate(fiedler) if f>=0])\n",
        "H2 = G.subgraph([i for (i,f) in enumerate(fiedler) if -f>=0])\n",
        "H = nx.union(H1,H2)\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "nx.draw_networkx(H, pos=nx.spring_layout(G, seed=42), with_labels=True)"
      ],
      "metadata": {
        "id": "g1PjOEj3vK1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(G, color=[fiedler>=0], cmap=\"Set2\")"
      ],
      "metadata": {
        "id": "K853z8t9vK4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fiedler_c = np.sort([biclasses,fiedler], axis=1)\n",
        "fiedler_1 = [v for (c,v) in np.transpose(fiedler_c) if c==1]\n",
        "l1 = len(fiedler_1)\n",
        "fiedler_0 = [v for (c,v) in np.transpose(fiedler_c) if c==0]\n",
        "l0 = len(fiedler_0)\n",
        "plt.plot(range(l0),fiedler_0,'o',color='red')\n",
        "plt.plot(range(l0,l1+l0),fiedler_1,'o',color='grey')\n",
        "plt.plot([0]*35)"
      ],
      "metadata": {
        "id": "Kc7BzfxrvK7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(12345)\n",
        "model = GCN()\n",
        "W1 = model.conv1.weight.detach().numpy()\n",
        "W2 = model.conv2.weight.detach().numpy()\n",
        "W3 = model.conv3.weight.detach().numpy()\n",
        "\n",
        "iteration = S**3*W1*W2*W3\n",
        "visualize(torch.tensor(iteration), color=biclasses)"
      ],
      "metadata": {
        "id": "YkoHIBASvK-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regr = linear_model.LinearRegression()\n",
        "regr.fit(iteration[:,0].reshape(-1, 1), iteration[:,1])\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "h = np.array(iteration)\n",
        "plt.scatter(h[:, 0], h[:, 1], s=140, c=biclasses, cmap=\"Set1\")\n",
        "plt.plot(h[:, 0],regr.predict(iteration[:,0].reshape(-1, 1)))"
      ],
      "metadata": {
        "id": "xq10qvH-vLC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def glorot_normal(in_c,out_c):\n",
        "    sigma = np.sqrt(2/(in_c+out_c))\n",
        "    return sigma*np.random.randn(in_c,out_c)"
      ],
      "metadata": {
        "id": "fPw1HEIzvhJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coef = np.zeros(n_simu)\n",
        "base =  np.zeros(n_simu)\n",
        "for i in range(n_simu):\n",
        "    iteration = glorot_normal(34,4)@glorot_normal(4,4)@glorot_normal(4,2)\n",
        "    regr.fit(iteration[:,0].reshape(-1, 1), iteration[:,1])\n",
        "    base[i] = mean_squared_error(iteration[:,1],regr.predict(iteration[:,0].reshape(-1, 1)))\n",
        "    iteration = np.array(S**3) @ iteration\n",
        "    regr.fit(iteration[:,0].reshape(-1, 1), iteration[:,1])\n",
        "    coef[i] = mean_squared_error(iteration[:,1],regr.predict(iteration[:,0].reshape(-1, 1)))"
      ],
      "metadata": {
        "id": "AgcGQrCqvhM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = plt.hist(base, bins = 34)\n",
        "_ = plt.hist(coef, bins = 34)"
      ],
      "metadata": {
        "id": "XVfriXttvhP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph ConvNets"
      ],
      "metadata": {
        "id": "72mS_2Vxw7lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget www.di.ens.fr/~lelarge/graphs.tar.gz\n",
        "# !tar -zxvf graphs.tar.gz\n",
        "# %cd graphs"
      ],
      "metadata": {
        "id": "ApoR9ox7vhUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_mnist_dataset_exists(path_data='./'):\n",
        "    flag_train_data = os.path.isfile(path_data + 'mnist/train_data.pt')\n",
        "    flag_train_label = os.path.isfile(path_data + 'mnist/train_label.pt')\n",
        "    flag_test_data = os.path.isfile(path_data + 'mnist/test_data.pt')\n",
        "    flag_test_label = os.path.isfile(path_data + 'mnist/test_label.pt')\n",
        "    if flag_train_data==False or flag_train_label==False or flag_test_data==False or flag_test_label==False:\n",
        "        print('MNIST dataset preprocessing...')\n",
        "        import torchvision\n",
        "        import torchvision.transforms as transforms\n",
        "        trainset = torchvision.datasets.MNIST(root=path_data + 'mnist/temp', train=True,\n",
        "                                                download=True, transform=transforms.ToTensor())\n",
        "        testset = torchvision.datasets.MNIST(root=path_data + 'mnist/temp', train=False,\n",
        "                                               download=True, transform=transforms.ToTensor())\n",
        "        train_data=torch.Tensor(60000,28,28)\n",
        "        train_label=torch.LongTensor(60000)\n",
        "        for idx , example in enumerate(trainset):\n",
        "            train_data[idx]=example[0].squeeze()\n",
        "            train_label[idx]=example[1]\n",
        "        torch.save(train_data,path_data + 'mnist/train_data.pt')\n",
        "        torch.save(train_label,path_data + 'mnist/train_label.pt')\n",
        "        test_data=torch.Tensor(10000,28,28)\n",
        "        test_label=torch.LongTensor(10000)\n",
        "        for idx , example in enumerate(testset):\n",
        "            test_data[idx]=example[0].squeeze()\n",
        "            test_label[idx]=example[1]\n",
        "        torch.save(test_data,path_data + 'mnist/test_data.pt')\n",
        "        torch.save(test_label,path_data + 'mnist/test_label.pt')\n",
        "    return path_data\n",
        "\n",
        "_ = check_mnist_dataset_exists()"
      ],
      "metadata": {
        "id": "dLO86irpvhXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# play with a small dataset (for cpu), uncomment.\n",
        "# nb_selected_train_data = 500\n",
        "# nb_selected_test_data = 100\n",
        "\n",
        "train_data=torch.load('mnist/train_data.pt').reshape(60000,784).numpy()\n",
        "#train_data = train_data[:nb_selected_train_data,:]\n",
        "print(train_data.shape)\n",
        "\n",
        "train_labels=torch.load('mnist/train_label.pt').numpy()\n",
        "#train_labels = train_labels[:nb_selected_train_data]\n",
        "print(train_labels.shape)\n",
        "\n",
        "test_data=torch.load('mnist/test_data.pt').reshape(10000,784).numpy()\n",
        "#test_data = test_data[:nb_selected_test_data,:]\n",
        "print(test_data.shape)\n",
        "\n",
        "test_labels=torch.load('mnist/test_label.pt').numpy()\n",
        "#test_labels = test_labels[:nb_selected_test_data]\n",
        "print(test_labels.shape)"
      ],
      "metadata": {
        "id": "BCui-UANxf4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct graph\n",
        "t_start = time.time()\n",
        "grid_side = 28\n",
        "number_edges = 8\n",
        "metric = 'euclidean'\n",
        "A = grid_graph(grid_side,number_edges,metric) # create graph of Euclidean grid"
      ],
      "metadata": {
        "id": "hgfyIL90xf60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def laplacian(W, normalized=True):\n",
        "    \"\"\"Return graph Laplacian\"\"\"\n",
        "    I = scipy.sparse.identity(W.shape[0], dtype=W.dtype)\n",
        "    # W += I\n",
        "    # degree matrix.\n",
        "    d = W.sum(axis=0)\n",
        "\n",
        "    # paplacian matrix.\n",
        "    if not normalized:\n",
        "        D = scipy.sparse.diags(d.A.squeeze(), 0)\n",
        "        L = D - W\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    assert np.abs(L - L.T).mean() < 1e-8\n",
        "    assert type(L) is scipy.sparse.csr.csr_matrix\n",
        "    return L"
      ],
      "metadata": {
        "id": "lRLksI5KxgAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rescale_L(L, lmax=2):\n",
        "    \"\"\"Rescale Laplacian eigenvalues to [-1,1]\"\"\"\n",
        "    M, M = L.shape\n",
        "    I = scipy.sparse.identity(M, format='csr', dtype=L.dtype)\n",
        "    L /= lmax * 2\n",
        "    L -= I\n",
        "    return L\n",
        "\n",
        "def lmax_L(L):\n",
        "    \"\"\"Compute largest Laplacian eigenvalue\"\"\"\n",
        "    return scipy.sparse.linalg.eigsh(L, k=1, which='LM', return_eigenvectors=False)[0]"
      ],
      "metadata": {
        "id": "uA4fKbRRxgEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coarsening_levels = 4\n",
        "\n",
        "L, perm = coarsen(A, coarsening_levels, partial(laplacian, normalized=False))\n",
        "\n",
        "# compute max eigenvalue of graph Laplacians\n",
        "lmax = []\n",
        "for i in range(coarsening_levels):\n",
        "    lmax.append(lmax_L(L[i]))\n",
        "print('lmax: ' + str([lmax[i] for i in range(coarsening_levels)]))\n",
        "\n",
        "# reindex nodes to satisfy a binary tree structure\n",
        "train_data = perm_data(train_data, perm)\n",
        "test_data = perm_data(test_data, perm)\n",
        "\n",
        "print('Execution time: {:.2f}s'.format(time.time() - t_start))\n",
        "del perm"
      ],
      "metadata": {
        "id": "R0AY9GUTxgH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Graph_ConvNet_LeNet5(nn.Module):\n",
        "\n",
        "    def __init__(self, net_parameters):\n",
        "\n",
        "        print('Graph ConvNet: LeNet5')\n",
        "\n",
        "        super(Graph_ConvNet_LeNet5, self).__init__()\n",
        "\n",
        "        # parameters\n",
        "        D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F = net_parameters\n",
        "        FC1Fin = CL2_F*(D//16)\n",
        "\n",
        "        # graph CL1\n",
        "        self.cl1 = nn.Linear(CL1_K, CL1_F)\n",
        "        self.init_layers(self.cl1, CL1_K, CL1_F)\n",
        "        self.CL1_K = CL1_K; self.CL1_F = CL1_F;\n",
        "\n",
        "        # graph CL2\n",
        "        self.cl2 = nn.Linear(CL2_K*CL1_F, CL2_F)\n",
        "        self.init_layers(self.cl2, CL2_K*CL1_F, CL2_F)\n",
        "        self.CL2_K = CL2_K; self.CL2_F = CL2_F;\n",
        "\n",
        "        # FC1\n",
        "        self.fc1 = nn.Linear(FC1Fin, FC1_F)\n",
        "        self.init_layers(self.fc1, FC1Fin, FC1_F)\n",
        "        self.FC1Fin = FC1Fin\n",
        "\n",
        "        # FC2\n",
        "        self.fc2 = nn.Linear(FC1_F, FC2_F)\n",
        "        self.init_layers(self.fc2, FC1_F, FC2_F)\n",
        "\n",
        "        # nb of parameters\n",
        "        nb_param = CL1_K* CL1_F + CL1_F          # CL1\n",
        "        nb_param += CL2_K* CL1_F* CL2_F + CL2_F  # CL2\n",
        "        nb_param += FC1Fin* FC1_F + FC1_F        # FC1\n",
        "        nb_param += FC1_F* FC2_F + FC2_F         # FC2\n",
        "        print('nb of parameters=',nb_param,'\\n')\n",
        "\n",
        "\n",
        "    def init_layers(self, W, Fin, Fout):\n",
        "\n",
        "        scale = np.sqrt( 2.0/ (Fin+Fout) )\n",
        "        W.weight.data.uniform_(-scale, scale)\n",
        "        W.bias.data.fill_(0.0)\n",
        "\n",
        "        return W\n",
        "\n",
        "\n",
        "    def graph_conv_cheby(self, x, cl, L, lmax, Fout, K):\n",
        "        # parameters\n",
        "        # B = batch size\n",
        "        # V = nb vertices\n",
        "        # Fin = nb input features\n",
        "        # Fout = nb output features\n",
        "        # K = Chebyshev order & support size\n",
        "        B, V, Fin = x.size(); B, V, Fin = int(B), int(V), int(Fin)\n",
        "\n",
        "        # rescale Laplacian\n",
        "        lmax = lmax_L(L)\n",
        "        L = rescale_L(L, lmax)\n",
        "\n",
        "        # convert scipy sparse matric L to pytorch\n",
        "        L = L.tocoo()\n",
        "        indices = np.column_stack((L.row, L.col)).T\n",
        "        indices = indices.astype(np.int64)\n",
        "        indices = torch.from_numpy(indices)\n",
        "        indices = indices.type(torch.LongTensor)\n",
        "        L_data = L.data.astype(np.float32)\n",
        "        L_data = torch.from_numpy(L_data)\n",
        "        L_data = L_data.type(torch.FloatTensor)\n",
        "        L = torch.sparse.FloatTensor(indices, L_data, torch.Size(L.shape))\n",
        "        L.requires_grad_(False)\n",
        "        if torch.cuda.is_available():\n",
        "            L = L.cuda()\n",
        "\n",
        "        # transform to Chebyshev basis\n",
        "        #\n",
        "        # inputs\n",
        "        # x B x V x Fin\n",
        "        # cl linear layer Fin*K x Fout\n",
        "        # L Laplacian lmax max eigenvalue\n",
        "        # output should be B x V x Fout\n",
        "        #\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    # max pooling of size p. Must be a power of 2.\n",
        "    def graph_max_pool(self, x, p):\n",
        "        #\n",
        "        # your code here\n",
        "        # input B x V x F output B x V/p x F\n",
        "        #\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, x, d, L, lmax):\n",
        "        # graph CL1\n",
        "        x = x.unsqueeze(2) # B x V x Fin=1\n",
        "        x = self.graph_conv_cheby(x, self.cl1, L[0], lmax[0], self.CL1_F, self.CL1_K)\n",
        "        x = F.relu(x)\n",
        "        x = self.graph_max_pool(x, 4)\n",
        "        # graph CL2\n",
        "        x = self.graph_conv_cheby(x, self.cl2, L[2], lmax[2], self.CL2_F, self.CL2_K)\n",
        "        x = F.relu(x)\n",
        "        x = self.graph_max_pool(x, 4)\n",
        "        # FC1\n",
        "        x = x.view(-1, self.FC1Fin)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x  = nn.Dropout(d)(x)\n",
        "        # FC2\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def loss(self, y, y_target, l2_regularization):\n",
        "\n",
        "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
        "\n",
        "        l2_loss = 0.0\n",
        "        for param in self.parameters():\n",
        "            data = param* param\n",
        "            l2_loss += data.sum()\n",
        "\n",
        "        loss += 0.5* l2_regularization* l2_loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def update(self, lr):\n",
        "\n",
        "        update = torch.optim.SGD( self.parameters(), lr=lr, momentum=0.9 )\n",
        "\n",
        "        return update\n",
        "\n",
        "\n",
        "    def update_learning_rate(self, optimizer, lr):\n",
        "\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "    def evaluation(self, y_predicted, test_l):\n",
        "\n",
        "        _, class_predicted = torch.max(y_predicted.data, 1)\n",
        "        return 100.0* (class_predicted == test_l).sum()/ y_predicted.size(0)\n",
        "\n",
        "# delete existing network if exists\n",
        "try:\n",
        "    del net\n",
        "    print('Delete existing network\\n')\n",
        "except NameError:\n",
        "    print('No existing network to delete\\n')\n",
        "\n",
        "# network parameters\n",
        "D = train_data.shape[1]\n",
        "CL1_F = 32\n",
        "CL1_K = 25\n",
        "CL2_F = 64\n",
        "CL2_K = 25\n",
        "FC1_F = 512\n",
        "FC2_F = 10\n",
        "net_parameters = [D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F]\n",
        "dropout_value = 0.5\n",
        "\n",
        "# instantiate the object net of the class\n",
        "net = Graph_ConvNet_LeNet5(net_parameters)\n",
        "if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "print(net)"
      ],
      "metadata": {
        "id": "LYB7lTEcxgLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y = train_data[:5,:], train_labels[:5]\n",
        "train_x =  torch.FloatTensor(train_x).type(dtypeFloat)\n",
        "train_y = train_y.astype(np.int64)\n",
        "train_y = torch.LongTensor(train_y).type(dtypeLong)\n",
        "\n",
        "# forward\n",
        "y = net(train_x, dropout_value, L, lmax)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "tGJmydI8yRqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# weights\n",
        "L_net = list(net.parameters())\n",
        "\n",
        "# learning parameters\n",
        "learning_rate = 0.05\n",
        "l2_regularization = 5e-4\n",
        "batch_size = 100\n",
        "num_epochs = 3\n",
        "train_size = train_data.shape[0]\n",
        "nb_iter = int(num_epochs * train_size) // batch_size\n",
        "print('num_epochs=',num_epochs,', train_size=',train_size,', nb_iter=',nb_iter)\n",
        "\n",
        "# optimizer\n",
        "global_lr = learning_rate\n",
        "global_step = 0\n",
        "decay = 0.95\n",
        "decay_steps = train_size\n",
        "lr = learning_rate\n",
        "optimizer = net.update(lr)\n",
        "\n",
        "# loop over epochs\n",
        "indices = collections.deque()\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    # reshuffle\n",
        "    indices.extend(np.random.permutation(train_size)) # rand permutation\n",
        "\n",
        "    # reset time\n",
        "    t_start = time.time()\n",
        "\n",
        "    # extract batches\n",
        "    running_loss = 0.0\n",
        "    running_accuray = 0\n",
        "    running_total = 0\n",
        "    while len(indices) >= batch_size:\n",
        "\n",
        "        # extract batches\n",
        "        batch_idx = [indices.popleft() for i in range(batch_size)]\n",
        "        train_x, train_y = train_data[batch_idx,:], train_labels[batch_idx]\n",
        "        train_x =  torch.FloatTensor(train_x).type(dtypeFloat)\n",
        "        train_y = train_y.astype(np.int64)\n",
        "        train_y = torch.LongTensor(train_y).type(dtypeLong)\n",
        "\n",
        "        # forward\n",
        "        y = net(train_x, dropout_value, L, lmax)\n",
        "        loss = net.loss(y,train_y,l2_regularization)\n",
        "        loss_train = loss.detach().item()\n",
        "        # accuracy\n",
        "        acc_train = net.evaluation(y,train_y.data)\n",
        "        # backward\n",
        "        loss.backward()\n",
        "        # Update\n",
        "        global_step += batch_size # to update learning rate\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # loss, accuracy\n",
        "        running_loss += loss_train\n",
        "        running_accuray += acc_train\n",
        "        running_total += 1\n",
        "        # print\n",
        "        if not running_total%100: # print every x mini-batches\n",
        "            print('epoch= %d, i= %4d, loss(batch)= %.4f, accuray(batch)= %.2f' % (epoch+1, running_total, loss_train, acc_train))\n",
        "\n",
        "    # print\n",
        "    t_stop = time.time() - t_start\n",
        "    print('epoch= %d, loss(train)= %.3f, accuracy(train)= %.3f, time= %.3f, lr= %.5f' %\n",
        "          (epoch+1, running_loss/running_total, running_accuray/running_total, t_stop, lr))\n",
        "\n",
        "    # update learning rate\n",
        "    lr = global_lr * pow( decay , float(global_step// decay_steps) )\n",
        "    optimizer = net.update_learning_rate(optimizer, lr)\n",
        "\n",
        "\n",
        "    # test set\n",
        "    with torch.no_grad():\n",
        "        running_accuray_test = 0\n",
        "        running_total_test = 0\n",
        "        indices_test = collections.deque()\n",
        "        indices_test.extend(range(test_data.shape[0]))\n",
        "        t_start_test = time.time()\n",
        "        while len(indices_test) >= batch_size:\n",
        "            batch_idx_test = [indices_test.popleft() for i in range(batch_size)]\n",
        "            test_x, test_y = test_data[batch_idx_test,:], test_labels[batch_idx_test]\n",
        "            test_x = torch.FloatTensor(test_x).type(dtypeFloat)\n",
        "            y = net(test_x, 0.0, L, lmax)\n",
        "            test_y = test_y.astype(np.int64)\n",
        "            test_y = torch.LongTensor(test_y).type(dtypeLong)\n",
        "            acc_test = net.evaluation(y,test_y.data)\n",
        "            running_accuray_test += acc_test\n",
        "            running_total_test += 1\n",
        "        t_stop_test = time.time() - t_start_test\n",
        "        print('  accuracy(test) = %.3f %%, time= %.3f' % (running_accuray_test / running_total_test, t_stop_test))"
      ],
      "metadata": {
        "id": "zqW7YgjKyRtv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}