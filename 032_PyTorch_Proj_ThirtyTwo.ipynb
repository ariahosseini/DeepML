{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oniCsy3JGXtu",
        "SBmPTdWpGRrD",
        "sEYU3RcoQX58",
        "gkf_6IgvQuX7",
        "CqGh4XKFZZ7v"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMcrHF8XN9ylyeCq+wy294i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariahosseini/DeepML/blob/main/032_PyTorch_Proj_ThirtyTwo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svu8bPpv49-s"
      },
      "outputs": [],
      "source": [
        "# utils\n",
        "import time, os, sys\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.special import binom, gamma\n",
        "from collections import OrderedDict\n",
        "# sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# vis\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def running_mean(x, N):\n",
        "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
        "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
        "\n",
        "def Catalan(k):\n",
        "    return binom(2*k,k)/(k+1)"
      ],
      "metadata": {
        "id": "6EQ1qgcAFLbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Using gpu: %s ' % torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLMS8MWAZqAf",
        "outputId": "9bccb856-52ff-46cf-a57a-23d273bc086c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using gpu: False \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "def gpu(tensor, gpu=use_gpu):\n",
        "    if gpu:\n",
        "        return tensor.cuda()\n",
        "    else:\n",
        "        return tensor"
      ],
      "metadata": {
        "id": "sd0NQu2nFLeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iUwoMSPAZryz",
        "outputId": "d4b836f6-d2dd-42df-fdd2-4a819a5760d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.0+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gen Data"
      ],
      "metadata": {
        "id": "oniCsy3JGXtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_max_len = 20\n",
        "seq_min_len = 4"
      ],
      "metadata": {
        "id": "JYpxSFboFpVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def all_parent(n, a, k=-1):\n",
        "    global res\n",
        "    if k==n-1 and sum(a) == 0:\n",
        "        res.append(a.copy())\n",
        "    elif k==n-1:\n",
        "        pass\n",
        "    else:\n",
        "        k += 1\n",
        "        if sum(a) > 0:\n",
        "            a[k] = 1\n",
        "            all_parent(n,a,k)\n",
        "            a[k] = -1\n",
        "            all_parent(n,a,k)\n",
        "            a[k] = 0\n",
        "        else:\n",
        "            a[k] = 1\n",
        "            all_parent(n,a,k)\n",
        "            a[k] = 0"
      ],
      "metadata": {
        "id": "lqsOTMtvFLiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def all_parent_mistake(n, a, k=-1):\n",
        "    global res\n",
        "    if k==n-1 and sum(a) >= -1 and sum(a) <= 1 and min(np.cumsum(a))<0:\n",
        "        res.append(a.copy())\n",
        "    elif sum(a) > n-k:\n",
        "        pass\n",
        "    elif k==n-1:\n",
        "        pass\n",
        "    else:\n",
        "        k += 1\n",
        "        if sum(a) >= -1 and k != 0:\n",
        "            a[k] = 1\n",
        "            all_parent_mistake(n,a,k)\n",
        "            a[k] = -1\n",
        "            all_parent_mistake(n,a,k)\n",
        "            a[k] = 0\n",
        "        else:\n",
        "            a[k] = 1\n",
        "            all_parent_mistake(n,a,k)\n",
        "            a[k] = 0"
      ],
      "metadata": {
        "id": "YEkOEDTPFLl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reading_par(l, n):\n",
        "    res = [0]*len(l)\n",
        "    s = []\n",
        "    n_plus = -1\n",
        "    n_moins = n+1\n",
        "    c = 0\n",
        "    for i in l:\n",
        "        if i == 1:\n",
        "            n_plus += 1\n",
        "            s.append(n_plus)\n",
        "            res[c] = n_plus\n",
        "            c += 1\n",
        "        else:\n",
        "            try:\n",
        "                res[c] = n-s.pop()\n",
        "            except:\n",
        "                res[c] = n_moins\n",
        "                n_moins += 1\n",
        "            c += 1\n",
        "    return res"
      ],
      "metadata": {
        "id": "4TC17DM7FLo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_par = OrderedDict()\n",
        "for n in range(seq_min_len,seq_max_len+1,2):\n",
        "    a = [0]*n\n",
        "    res = []\n",
        "    all_parent(n=n,a=a,k=-1)\n",
        "    all_par[n] = [reading_par(k,n) for k in res]"
      ],
      "metadata": {
        "id": "2QBvpVTxFLsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_par_mist = OrderedDict()\n",
        "for n in range(seq_min_len,seq_max_len+1,2):\n",
        "    a = [0]*n\n",
        "    res = []\n",
        "    all_parent_mistake(n=n,a=a,k=-1)\n",
        "    all_par_mist[n] = [reading_par(k,n) for k in res]"
      ],
      "metadata": {
        "id": "yAC4pJvyFLxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_par[6], all_par_mist[6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REHa6uKlFLzM",
        "outputId": "911b481a-5fb2-4333-aabe-d53e10c926dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[0, 1, 2, 4, 5, 6],\n",
              "  [0, 1, 5, 2, 4, 6],\n",
              "  [0, 1, 5, 6, 2, 4],\n",
              "  [0, 6, 1, 2, 4, 5],\n",
              "  [0, 6, 1, 5, 2, 4]],\n",
              " [[0, 1, 5, 6, 7, 2],\n",
              "  [0, 6, 1, 5, 7, 2],\n",
              "  [0, 6, 7, 1, 2, 4],\n",
              "  [0, 6, 7, 1, 5, 2],\n",
              "  [0, 6, 7, 8, 1, 2]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "long_mist = {i:len(l) for (i,l) in zip(all_par_mist.keys(), all_par_mist.values())}\n",
        "long_mist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5l5dHjOFL2T",
        "outputId": "d03610ef-2fe5-41fc-85a8-cf78c366dc8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{4: 1, 6: 5, 8: 20, 10: 75, 12: 275, 14: 1001, 16: 3640, 18: 13260, 20: 48450}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Catalan_num = {i:len(l) for (i,l) in zip(all_par.keys(),all_par.values())}\n",
        "Catalan_num"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y17N72u5FL-a",
        "outputId": "1062d60b-26ae-446b-d472-ccc9d478fd2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{4: 2, 6: 5, 8: 14, 10: 42, 12: 132, 14: 429, 16: 1430, 18: 4862, 20: 16796}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[(2*i,Catalan(i)) for i  in range(2,int(seq_max_len/2)+1)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua3JviaqFMDR",
        "outputId": "84005ec8-0656-4935-cfaa-d6ca7b30fc31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(4, 2.0),\n",
              " (6, 5.0),\n",
              " (8, 14.0),\n",
              " (10, 42.0),\n",
              " (12, 132.0),\n",
              " (14, 429.0),\n",
              " (16, 1430.0),\n",
              " (18, 4862.0),\n",
              " (20, 16796.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nb_symbol = 10\n",
        "np.sum([Catalan(i)*int(nb_symbol/2)**i for i in range(2,int(seq_max_len/2)+1)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nn3KiciFMIA",
        "outputId": "b9982349-bcd7-4263-96e4-4c45734ec7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "174113843800.0"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceGenerator():\n",
        "    def __init__(self, nb_symbol = 10, seq_min_len = 4, seq_max_len = 10):\n",
        "        self.nb_symbol = nb_symbol\n",
        "        self.seq_min_len = seq_min_len\n",
        "        self.seq_max_len = seq_max_len\n",
        "        self.population = [i for i in range(int(nb_symbol/2))]\n",
        "\n",
        "    def generate_pattern(self):\n",
        "        len_r = random.randint(self.seq_min_len/2,self.seq_max_len/2)\n",
        "        pattern = random.choices(self.population,k=len_r)\n",
        "        return pattern + pattern[::-1]\n",
        "\n",
        "    def generate_pattern_parenthesis(self, len_r = None):\n",
        "        if len_r == None:\n",
        "            len_r = int(2*random.randint(self.seq_min_len/2,self.seq_max_len/2))\n",
        "        pattern = np.random.choice(self.population,size=int(len_r/2),replace=True)\n",
        "        ind_r = random.randint(0,Catalan_num[len_r]-1)\n",
        "        res = [pattern[i] if i <= len_r/2 else self.nb_symbol-1-pattern[len_r-i] for i in all_par[len_r][ind_r]]\n",
        "        return res\n",
        "\n",
        "    def generate_parenthesis_false(self):\n",
        "        len_r = int(2*random.randint(self.seq_min_len/2,self.seq_max_len/2))\n",
        "        pattern = np.random.choice(self.population,size=int(len_r/2),replace=True)\n",
        "        ind_r = random.randint(0,long_mist[len_r]-1)\n",
        "        res = [pattern[i] if i <= len_r/2\n",
        "               else  self.nb_symbol-1-pattern[len_r-i] if i<= len_r\n",
        "               else self.nb_symbol-1-pattern[i-len_r] for i in all_par_mist[len_r][ind_r]]\n",
        "        return res\n",
        "\n",
        "    def generate_hard_parenthesis(self, len_r = None):\n",
        "        if len_r == None:\n",
        "            len_r = int(2*random.randint(self.seq_min_len/2,self.seq_max_len/2))\n",
        "        pattern = np.random.choice(self.population,size=int(len_r/2),replace=True)\n",
        "        ind_r = random.randint(0,Catalan_num[len_r]-1)\n",
        "        res = [pattern[i] if i <= len_r/2 else self.nb_symbol-1-pattern[len_r-i] for i in all_par[len_r][ind_r]]\n",
        "\n",
        "        if len_r == None:\n",
        "            len_r = int(2*random.randint(self.seq_min_len/2,self.seq_max_len/2))\n",
        "        pattern = np.random.choice(self.population,size=int(len_r/2),replace=True)\n",
        "        ind_r = random.randint(0,Catalan_num[len_r]-1)\n",
        "        res2 = [pattern[i] if i <= len_r/2 else self.nb_symbol-1-pattern[len_r-i] for i in all_par[len_r][ind_r]]\n",
        "        return res + res2\n",
        "\n",
        "    def generate_hard_nonparenthesis(self, len_r = None):\n",
        "        if len_r == None:\n",
        "            len_r = int(2*random.randint(self.seq_min_len/2,self.seq_max_len/2))\n",
        "        pattern = np.random.choice(self.population,size=int(len_r/2),replace=True)\n",
        "        ind_r = random.randint(0,long_mist[len_r]-1)\n",
        "        res = [pattern[i] if i <= len_r/2\n",
        "               else  self.nb_symbol-1-pattern[len_r-i] if i<= len_r\n",
        "               else self.nb_symbol-1-pattern[i-len_r] for i in all_par_mist[len_r][ind_r]]\n",
        "\n",
        "        if len_r == None:\n",
        "            len_r = int(2*random.randint(self.seq_min_len/2,self.seq_max_len/2))\n",
        "        pattern = np.random.choice(self.population,size=int(len_r/2),replace=True)\n",
        "        ind_r = random.randint(0,Catalan_num[len_r]-1)\n",
        "        res2 = [pattern[i] if i <= len_r/2 else self.nb_symbol-1-pattern[len_r-i] for i in all_par[len_r][ind_r]]\n",
        "        return  res +[self.nb_symbol-1-pattern[0]]+ res2\n",
        "\n",
        "    def generate_false(self):\n",
        "        popu = [i for i in range(nb_symbol)]\n",
        "        len = random.randint(self.seq_min_len/2,self.seq_max_len/2)\n",
        "        return random.choices(popu,k=len) + random.choices(popu,k=len)\n",
        "\n",
        "    def generate_label(self, x):\n",
        "        l = int(len(x)/2)\n",
        "        return 1 if x[:l] == x[:l-1:-1] else 0\n",
        "\n",
        "    def generate_label_parenthesis(self, x):\n",
        "        s = []\n",
        "        label = 1\n",
        "        lenx = len(x)\n",
        "        for i in x:\n",
        "            if s == [] and i < self.nb_symbol/2:\n",
        "                s.append(i)\n",
        "            elif s == [] and i >= self.nb_symbol/2:\n",
        "                label = 0\n",
        "                break\n",
        "            elif i == self.nb_symbol-1-s[-1]:\n",
        "                s.pop()\n",
        "            else:\n",
        "                s.append(i)\n",
        "        if s != []:\n",
        "            label = 0\n",
        "        return label\n",
        "\n",
        "    def one_hot(self,seq):\n",
        "        one_hot_seq = []\n",
        "        for s in seq:\n",
        "            one_hot = [0 for _ in range(self.nb_symbol)]\n",
        "            one_hot[s] = 1\n",
        "            one_hot_seq.append(one_hot)\n",
        "        return one_hot_seq\n",
        "\n",
        "    def generate_input(self, len_r = None, true_parent = False, hard_false = True):\n",
        "        if true_parent:\n",
        "            seq = self.generate_pattern_parenthesis(len_r)\n",
        "        elif bool(random.getrandbits(1)):\n",
        "            seq = self.generate_pattern_parenthesis(len_r)\n",
        "        else:\n",
        "            if hard_false:\n",
        "                seq = self.generate_parenthesis_false()\n",
        "            else:\n",
        "                seq = self.generate_false()\n",
        "        return gpu(torch.from_numpy(np.array(self.one_hot(seq))).type(torch.FloatTensor)), gpu(torch.from_numpy(np.array([self.generate_label_parenthesis(seq)])))\n",
        "\n",
        "    def generate_input_hard(self,true_parent = False):\n",
        "        if true_parent:\n",
        "            seq = self.generate_hard_parenthesis(self.seq_max_len)\n",
        "        elif bool(random.getrandbits(1)):\n",
        "            seq = self.generate_hard_parenthesis(self.seq_max_len)\n",
        "        else:\n",
        "            seq = self.generate_hard_nonparenthesis(self.seq_max_len)\n",
        "\n",
        "        return gpu(torch.from_numpy(np.array(self.one_hot(seq))).type(torch.FloatTensor)), gpu(torch.from_numpy(np.array([self.generate_label_parenthesis(seq)])))"
      ],
      "metadata": {
        "id": "_KCku1ulFMKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_symbol = 10\n",
        "generator = SequenceGenerator(nb_symbol = nb_symbol, seq_min_len = seq_min_len, seq_max_len = seq_max_len)"
      ],
      "metadata": {
        "id": "83uaPGdyFMNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.generate_pattern_parenthesis()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm2CIAmrFMRa",
        "outputId": "ca9e6984-af5f-41f1-cc45-3a088b993fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 0, 9, 2, 4, 5, 7, 6, 1, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = generator.generate_parenthesis_false()"
      ],
      "metadata": {
        "id": "HiT3h0kJGMKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.generate_label_parenthesis(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seqTJ8-sGMOK",
        "outputId": "9735f202-8478-4722-c6c9-7c5bcef1d8fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator.generate_input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UvOz6GrGMR_",
        "outputId": "f14ac1b7-4415-4c99-bea4-592e70a51b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]),\n",
              " tensor([1]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Elman Network"
      ],
      "metadata": {
        "id": "SBmPTdWpGRrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RecNet(nn.Module):\n",
        "    def __init__(self, dim_input=10, dim_recurrent=50, dim_output=2):\n",
        "        super(RecNet, self).__init__()\n",
        "        self.fc_x2h = nn.Linear(dim_input, dim_recurrent)\n",
        "        self.fc_h2h = nn.Linear(dim_recurrent, dim_recurrent, bias = False)\n",
        "        self.fc_h2y = nn.Linear(dim_recurrent, dim_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x.new_zeros(1, self.fc_h2y.weight.size(1))\n",
        "        for t in range(x.size(0)):\n",
        "            h = torch.relu(self.fc_x2h(x[t,:]) + self.fc_h2h(h))\n",
        "        return self.fc_h2y(h)\n",
        "\n",
        "RNN = gpu(RecNet(dim_input = nb_symbol))"
      ],
      "metadata": {
        "id": "l6Nwub5wP-EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.Adam(RNN.parameters(),lr=learning_rate)\n",
        "nb_train = 40000\n",
        "loss_t = []\n",
        "corrects =[]\n",
        "labels = []\n",
        "start = time.time()\n",
        "for k in range(nb_train):\n",
        "    x,l = generator.generate_input(hard_false = False)\n",
        "    y = RNN(x)\n",
        "    loss = cross_entropy(y,l)\n",
        "    _,preds = torch.max(y.data,1)\n",
        "    corrects.append(preds.item() == l.data.item())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_t.append(loss)\n",
        "    labels.append(l.data)\n",
        "print(time.time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD0gdU2FP-I3",
        "outputId": "d09694ff-bfda-4931-f2f0-352a894a5506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "142.6707227230072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(running_mean(loss_t, int(nb_train/100)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "GjTVx6zzP-OA",
        "outputId": "33031fee-e90e-4b8c-f2ae-a65afc2b906f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-2f428293c6e6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_train\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-1a0d5c4bd488>\u001b[0m in \u001b[0;36mrunning_mean\u001b[0;34m(x, N)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcumsum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcumsum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mCatalan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(arr, obj, values, axis)\u001b[0m\n\u001b[1;32m   5462\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5464\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5465\u001b[0m     \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5466\u001b[0m     \u001b[0marrorder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'F'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnc\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1085\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(running_mean(corrects,int(nb_train/100)))"
      ],
      "metadata": {
        "id": "zgN2E3AEP-SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_test = 1000\n",
        "corrects_test =[]\n",
        "labels_test = []\n",
        "for k in range(nb_test):\n",
        "    x,l = generator.generate_input(len_r=seq_max_len, true_parent=True)\n",
        "    y = RNN(x)\n",
        "    _,preds = torch.max(y.data,1)\n",
        "    corrects_test.append(preds.item() == l.data.item())\n",
        "    labels_test.append(l.data)"
      ],
      "metadata": {
        "id": "ca1HDc18P-Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(corrects_test)/nb_test"
      ],
      "metadata": {
        "id": "j10s0TENP-k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_test = 1000\n",
        "corrects_test =[]\n",
        "labels_test = []\n",
        "for k in range(nb_test):\n",
        "    x,l = generator.generate_input(len_r=seq_max_len, hard_false=True)\n",
        "    y = RNN(x)\n",
        "    _,preds = torch.max(y.data,1)\n",
        "    corrects_test.append(preds.item() == l.data.item())\n",
        "    labels_test.append(l.data)"
      ],
      "metadata": {
        "id": "maMHBVEqP-op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(corrects_test)/nb_test"
      ],
      "metadata": {
        "id": "j8Mi5XyOP-ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_test = 1000\n",
        "correctsh_test =[]\n",
        "labelsh_test = []\n",
        "for k in range(nb_test):\n",
        "    x,l = generator.generate_input_hard()\n",
        "    y = RNN(x)\n",
        "    _,preds = torch.max(y.data,1)\n",
        "    correctsh_test.append(preds.item() == l.data.item())\n",
        "    labelsh_test.append(l.data)"
      ],
      "metadata": {
        "id": "8r55icTEP-yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(correctsh_test)/nb_test"
      ],
      "metadata": {
        "id": "Cvp4p48KQS56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_test = 1000\n",
        "correctsh_test =[]\n",
        "labelsh_test = []\n",
        "for k in range(nb_test):\n",
        "    x,l = generator.generate_input_hard(true_parent=True)\n",
        "    y = RNN(x)\n",
        "    _,preds = torch.max(y.data,1)\n",
        "    correctsh_test.append(preds.item() == l.data.item())\n",
        "    labelsh_test.append(l.data)"
      ],
      "metadata": {
        "id": "CxLd6x1iQTAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(correctsh_test)/nb_test"
      ],
      "metadata": {
        "id": "xAO9QxCbQTEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN with Gating"
      ],
      "metadata": {
        "id": "sEYU3RcoQX58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RecNetGating(nn.Module):\n",
        "    def __init__(self, dim_input=10, dim_recurrent=50, dim_output=2):\n",
        "        super(RecNetGating, self).__init__()\n",
        "        self.fc_x2h = nn.Linear(dim_input, dim_recurrent)\n",
        "        self.fc_h2h = nn.Linear(dim_recurrent, dim_recurrent, bias = False)\n",
        "        self.fc_x2z = nn.Linear(dim_input, dim_recurrent)\n",
        "        self.fc_h2z = nn.Linear(dim_recurrent,dim_recurrent, bias = False)\n",
        "        self.fc_h2y = nn.Linear(dim_recurrent, dim_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x.new_zeros(1, self.fc_h2y.weight.size(1))\n",
        "        for t in range(x.size(0)):\n",
        "            z = torch.sigmoid(self.fc_x2z(x[t,:])+self.fc_h2z(h))\n",
        "            hb = torch.relu(self.fc_x2h(x[t,:]) + self.fc_h2h(h))\n",
        "            h = z * h + (1-z) * hb\n",
        "        return self.fc_h2y(h)\n",
        "\n",
        "RNNG = gpu(RecNetGating(dim_input = nb_symbol))"
      ],
      "metadata": {
        "id": "5Pfp6JarQTI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizerG = torch.optim.Adam(RNNG.parameters(),lr=1e-3)\n",
        "loss_tG = []\n",
        "correctsG =[]\n",
        "labelsG = []\n",
        "start = time.time()\n",
        "for k in range(nb_train):\n",
        "    x,l = generator.generate_input(hard_false = False)\n",
        "    y = RNNG(x)\n",
        "    loss = cross_entropy(y,l)\n",
        "    _,preds = torch.max(y.data,1)\n",
        "    correctsG.append(preds.item() == l.data.item())\n",
        "    optimizerG.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizerG.step()\n",
        "    loss_tG.append(loss)\n",
        "    labelsG.append(l.item())\n",
        "print(time.time() - start)"
      ],
      "metadata": {
        "id": "yIsd2NLcQTMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(running_mean(loss_tG, int(nb_train/50)))\n",
        "plt.plot(running_mean(loss_t, int(nb_train/50)))"
      ],
      "metadata": {
        "id": "-xFH2oVFQTQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(running_mean(correctsG, int(nb_train/50)))\n",
        "plt.plot(running_mean(corrects, int(nb_train/50)))"
      ],
      "metadata": {
        "id": "F5bAY4-cQTVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_test = 1000\n",
        "correctsG_test =[]\n",
        "labelsG_test = []\n",
        "for k in range(nb_test):\n",
        "    x,l = generator.generate_input(len_r=seq_max_len,true_parent=True)\n",
        "    y = RNNG(x)\n",
        "    _,preds = torch.max(y.data,1)\n",
        "    correctsG_test.append(preds.item() == l.data.item())\n",
        "    labelsG_test.append(l.data)"
      ],
      "metadata": {
        "id": "Jhn2xzS_QTZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(correctsG_test)/nb_test"
      ],
      "metadata": {
        "id": "qGD0NHyTQTd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_test = 1000\n",
        "correctsG_test =[]\n",
        "labelsG_test = []\n",
        "for k in range(nb_test):\n",
        "    x,l = generator.generate_input(len_r=seq_max_len, hard_false = True)\n",
        "    y = RNNG(x)\n",
        "    _,preds = torch.max(y.data,1)\n",
        "    correctsG_test.append(preds.item() == l.data.item())\n",
        "    labelsG_test.append(l.data)"
      ],
      "metadata": {
        "id": "kAkXy7_jQTit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(correctsG_test)/nb_test"
      ],
      "metadata": {
        "id": "97Gjq-uuQpD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_test = 1000\n",
        "correctshG_test =[]\n",
        "labelshG_test = []\n",
        "for k in range(nb_test):\n",
        "    x,l = generator.generate_input_hard()\n",
        "    y = RNNG(x)\n",
        "    _,preds = torch.max(y.data,1)\n",
        "    correctshG_test.append(preds.item() == l.data.item())\n",
        "    labelshG_test.append(l.data)"
      ],
      "metadata": {
        "id": "So_0Vk3vQpIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(correctshG_test)/nb_test"
      ],
      "metadata": {
        "id": "114-rm9YQpLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "gkf_6IgvQuX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, dim_input=10, dim_recurrent=50, num_layers=4, dim_output=2):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size = dim_input,\n",
        "                           hidden_size = dim_recurrent,\n",
        "                           num_layers = num_layers)\n",
        "        self.fc_o2y = nn.Linear(dim_recurrent,dim_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        output, _ = self.lstm(x)\n",
        "        output = output.squeeze(1)\n",
        "        output = output.narrow(0, output.size(0)-1,1)\n",
        "        return self.fc_o2y(F.relu(output))\n",
        "\n",
        "lstm = gpu(LSTMNet(dim_input = nb_symbol))"
      ],
      "metadata": {
        "id": "-bWG38C9QpQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, l = generator.generate_input()"
      ],
      "metadata": {
        "id": "jT5Sr3nHQpdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm(x)"
      ],
      "metadata": {
        "id": "h2sEbeu2QpjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizerL = torch.optim.Adam(lstm.parameters(),lr=1e-3)\n",
        "loss_tL = []\n",
        "correctsL =[]\n",
        "labelsL = []\n",
        "start = time.time()\n",
        "for k in range(nb_train):\n",
        "    x,l = generator.generate_input(hard_false = False)\n",
        "    y = lstm(x)\n",
        "    loss = cross_entropy(y,l)\n",
        "    _,preds = torch.max(y.data,1)\n",
        "    correctsL.append(preds.item() == l.data.item())\n",
        "    optimizerL.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizerL.step()\n",
        "    loss_tL.append(loss)\n",
        "    labelsL.append(l.item())\n",
        "print(time.time() - start)"
      ],
      "metadata": {
        "id": "aIXibX6bQppr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(running_mean(loss_tL,int(nb_train/50)))\n",
        "plt.plot(running_mean(loss_tG,int(nb_train/50)))\n",
        "plt.plot(running_mean(loss_t,int(nb_train/50)))"
      ],
      "metadata": {
        "id": "4pyFhM7xQptq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(running_mean(correctsL,int(nb_train/50)))\n",
        "plt.plot(running_mean(correctsG,int(nb_train/50)))\n",
        "plt.plot(running_mean(corrects,int(nb_train/50)))"
      ],
      "metadata": {
        "id": "Y4aIcYv6Qp0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_test = 1000\n",
        "correctsL_test =[]\n",
        "labelsL_test = []\n",
        "for k in range(nb_test):\n",
        "    x,l = generator.generate_input(len_r=seq_max_len, true_parent=True)\n",
        "    y = lstm(x)\n",
        "    _,preds = torch.max(y.data, 1)\n",
        "    correctsL_test.append(preds.item() == l.data.item())\n",
        "    labelsL_test.append(l.data)"
      ],
      "metadata": {
        "id": "PrEomqarQp6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(correctsL_test)/nb_test"
      ],
      "metadata": {
        "id": "ZMEI8b5HQ6s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_test = 1000\n",
        "correctsL_test =[]\n",
        "labelsL_test = []\n",
        "for k in range(nb_test):\n",
        "    x,l = generator.generate_input(len_r=seq_max_len,true_parent=False,hard_false = True)\n",
        "    y = lstm(x)\n",
        "    _,preds = torch.max(y.data,1)\n",
        "    correctsL_test.append(preds.item() == l.data.item())\n",
        "    labelsL_test.append(l.data)"
      ],
      "metadata": {
        "id": "cGH-gOXKQ6w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(correctsL_test)/nb_test"
      ],
      "metadata": {
        "id": "76wbDEbjQ61N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_test = 1000\n",
        "correctshL_test =[]\n",
        "labelshL_test = []\n",
        "for k in range(nb_test):\n",
        "    x,l = generator.generate_input_hard()\n",
        "    y = lstm(x)\n",
        "    _,preds = torch.max(y.data,1)\n",
        "    correctshL_test.append(preds.item() == l.data.item())\n",
        "    labelshL_test.append(l.data)"
      ],
      "metadata": {
        "id": "ZJU6nw0eQ640"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(correctshL_test)/nb_test"
      ],
      "metadata": {
        "id": "7TQCv8K5Q68q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pred Engine Failure RNN"
      ],
      "metadata": {
        "id": "CqGh4XKFZZ7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir data\n",
        "%cd data\n",
        "!wget 'https://www.di.ens.fr/~lelarge/CMAPSSData.zip'\n",
        "!unzip CMAPSSData.zip\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_QnsAm9Zf9F",
        "outputId": "eb6c8a21-0713-40de-f5aa-b6b440109771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data/data\n",
            "--2024-05-21 23:07:07--  https://www.di.ens.fr/~lelarge/CMAPSSData.zip\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘CMAPSSData.zip’\n",
            "\n",
            "CMAPSSData.zip          [        <=>         ]  11.85M  6.62MB/s    in 1.8s    \n",
            "\n",
            "2024-05-21 23:07:09 (6.62 MB/s) - ‘CMAPSSData.zip’ saved [12425978]\n",
            "\n",
            "Archive:  CMAPSSData.zip\n",
            "  inflating: Damage Propagation Modeling.pdf  \n",
            "  inflating: readme.txt              \n",
            "  inflating: RUL_FD001.txt           \n",
            "  inflating: RUL_FD002.txt           \n",
            "  inflating: RUL_FD003.txt           \n",
            "  inflating: RUL_FD004.txt           \n",
            "  inflating: test_FD001.txt          \n",
            "  inflating: test_FD002.txt          \n",
            "  inflating: test_FD003.txt          \n",
            "  inflating: test_FD004.txt          \n",
            "  inflating: train_FD001.txt         \n",
            "  inflating: train_FD002.txt         \n",
            "  inflating: train_FD003.txt         \n",
            "  inflating: train_FD004.txt         \n",
            "/content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_CMAPSSData(nb_file):\n",
        "    dataset_train = pd.read_csv('./data/train_FD00{}.txt'.format(nb_file),\n",
        "                                sep=' ', header=None).drop([26, 27], axis=1)\n",
        "    dataset_test = pd.read_csv('./data/test_FD00{}.txt'.format(nb_file),\n",
        "                               sep=' ', header=None).drop([26, 27], axis=1)\n",
        "    test_truth = pd.read_csv('./data/RUL_FD00{}.txt'.format(nb_file),\n",
        "                             sep=' ', header=None).drop([1], axis=1)\n",
        "    col_names = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8',\n",
        "                 's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "    dataset_train.columns = col_names\n",
        "    dataset_test.columns = col_names\n",
        "    test_truth.columns = ['more']\n",
        "    test_truth['id'] = test_truth.index + 1\n",
        "    rul = pd.DataFrame(dataset_test.groupby('id')['cycle'].max()).reset_index()\n",
        "    rul.columns = ['id', 'max']\n",
        "    test_truth['rtf'] = test_truth['more'] + rul['max']\n",
        "    test_truth.drop('more', axis=1, inplace=True)\n",
        "    dataset_test = dataset_test.merge(test_truth, on=['id'], how='left')\n",
        "    dataset_test['ttf'] = dataset_test['rtf'] - dataset_test['cycle']\n",
        "    dataset_test.drop('rtf', axis=1, inplace=True)\n",
        "    dataset_train['ttf'] = dataset_train.groupby(['id'])['cycle'].transform(max) - dataset_train['cycle']\n",
        "    features_col_name = ['setting1', 'setting2', 'setting3', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8',\n",
        "                         's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "    target_col_name = 'ttf'\n",
        "    relevant_features_col_name = []\n",
        "    for col in features_col_name:\n",
        "        if not (len(dataset_train[col].unique()) == 1):\n",
        "            relevant_features_col_name.append(col)\n",
        "    sc = MinMaxScaler()\n",
        "    dataset_train[features_col_name] = sc.fit_transform(dataset_train[features_col_name])\n",
        "    dataset_test[features_col_name] = sc.transform(dataset_test[features_col_name])\n",
        "    return dataset_train, dataset_test, relevant_features_col_name, target_col_name\n",
        "\n",
        "def to_lists_of_tensors(dataset, features_col_name, target_col_name):\n",
        "    X, y = [], []\n",
        "    nb_sequences = max(dataset['id'])\n",
        "    for i in range(1, nb_sequences + 1):\n",
        "        df_zeros = dataset.loc[dataset['id'] == i]\n",
        "        df_one_x = df_zeros[features_col_name]\n",
        "        df_one_y = df_zeros[target_col_name]\n",
        "        X.append(torch.from_numpy(np.expand_dims(df_one_x.values, 1)).type(torch.FloatTensor))\n",
        "        y.append(torch.from_numpy(df_one_y.values).type(torch.FloatTensor))\n",
        "    return X, y\n",
        "\n",
        "def convert_train_and_test_to_appropriate_format(dataset_train, dataset_test, features_col_name, target_col_name):\n",
        "    X_train, y_train = to_lists_of_tensors(dataset_train, features_col_name, target_col_name)\n",
        "    X_test, y_test = to_lists_of_tensors(dataset_test, features_col_name, target_col_name)\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbc-4pFDZgHw",
        "outputId": "78bb8d8a-a786-4a68-a21e-2f5fd0107975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CMAPSSData.zip\t\t\t    RUL_FD001.txt   test_FD001.txt   train_FD001.txt\n",
            "'Damage Propagation Modeling.pdf'   RUL_FD002.txt   test_FD002.txt   train_FD002.txt\n",
            " data\t\t\t\t    RUL_FD003.txt   test_FD003.txt   train_FD003.txt\n",
            " readme.txt\t\t\t    RUL_FD004.txt   test_FD004.txt   train_FD004.txt\n",
            " CMAPSSData.zip\t\t\t    RUL_FD002.txt    test_FD002.txt    train_FD002.txt\n",
            "'Damage Propagation Modeling.pdf'   RUL_FD003.txt    test_FD003.txt    train_FD003.txt\n",
            " readme.txt\t\t\t    RUL_FD004.txt    test_FD004.txt    train_FD004.txt\n",
            " RUL_FD001.txt\t\t\t    test_FD001.txt   train_FD001.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pycat ./data/readme.txt"
      ],
      "metadata": {
        "id": "m649LqTNZgQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train, dataset_test, features_col_name, target_col_name = get_CMAPSSData(1)\n",
        "X_train, y_train, X_test, y_test = convert_train_and_test_to_appropriate_format(dataset_train, dataset_test, features_col_name, target_col_name)"
      ],
      "metadata": {
        "id": "TK1PD_wQZgUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.head()"
      ],
      "metadata": {
        "id": "meEj7fsdZgYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0].shape"
      ],
      "metadata": {
        "id": "IObHJnkaZglW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUnet(nn.Module):\n",
        "    def __init__(self, dim_input, num_layers, dim_hidden, dim_output=2):\n",
        "        super(GRUnet, self).__init__()\n",
        "        self.gru = nn.GRU(input_size = dim_input,\n",
        "                          hidden_size = dim_hidden, num_layers = num_layers)\n",
        "        self.hidden_to_two = nn.Linear(dim_hidden, dim_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gru_output, _ = self.gru(x)\n",
        "        gru_output = gru_output.squeeze(1)\n",
        "        two_output = self.hidden_to_two(gru_output)\n",
        "        return torch.exp(two_output)"
      ],
      "metadata": {
        "id": "T0ifcGA2ZgpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GRUnet(dim_input=len(features_col_name), num_layers=3, dim_hidden=50)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "DJtGYTp5Q7Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(X_train[0].to(device))"
      ],
      "metadata": {
        "id": "0CTkx_2LQ7HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "id": "fg1e5PC7Q7KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class weibull_loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(weibull_loss, self).__init__()\n",
        "        self.epsilon = 1e-6\n",
        "\n",
        "    def forward(self, output, y):\n",
        "        ya = (y + self.epsilon) / (output[:, 0])\n",
        "        beta = output[:, 1]\n",
        "        likelihoods = torch.log(beta) + beta * torch.log(ya) - torch.log(y+self.epsilon)- ya ** beta\n",
        "        return -likelihoods.mean()"
      ],
      "metadata": {
        "id": "XuC70IXFQ7Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = weibull_loss()\n",
        "loss_fn(output.squeeze(), y_train[0].to(device))"
      ],
      "metadata": {
        "id": "13bcd3iBaor7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(X_train, y_train, model, loss_fn, optimizer, device):\n",
        "    nb_train_sequences = len(y_train)\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for k in range(nb_train_sequences):\n",
        "        X, y = X_train[k], y_train[k]\n",
        "        pred = model(X.to(device))\n",
        "        loss = loss_fn(pred.squeeze(), y.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test_epoch(X_test, y_test, model, loss_fn, device):\n",
        "    nb_test_sequences = len(y_test)\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for k in range(nb_test_sequences):\n",
        "        X, y = X_test[k], y_test[k]\n",
        "        pred = model(X.to(device))\n",
        "        loss = loss_fn(pred.squeeze(), y.to(device))\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "def fit(model, X_train, y_train, X_test, y_test, optimizer, loss_fn, nb_epochs, device):\n",
        "    train_loss_t = []\n",
        "    test_loss_t = []\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=1, verbose='True',\n",
        "                                                           threshold=0.001)\n",
        "    for epoch in range(0, nb_epochs):\n",
        "        message1 = 'Epoch: {}/{}'.format(epoch + 1, nb_epochs)\n",
        "        print(message1)\n",
        "        train_loss = train_epoch(X_train, y_train, model, loss_fn, optimizer,device)\n",
        "        test_loss = test_epoch(X_test, y_test, model, loss_fn,device)\n",
        "        message2 = 'Train set: Average loss: {:.4f}\\n'.format(train_loss)\n",
        "        message3 = 'Test set: Average loss: {:.4f} - Learning rate : {:.4f}\\n'.format(test_loss,optimizer.param_groups[0]['lr'])\n",
        "        scheduler.step(test_loss)\n",
        "        train_loss_t.append(train_loss)\n",
        "        test_loss_t.append(test_loss)\n",
        "        print(message2)\n",
        "        print(message3)\n",
        "    return model, train_loss_t, test_loss_t"
      ],
      "metadata": {
        "id": "WskcXWwSaovO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GRUnet(dim_input=len(features_col_name), num_layers=3, dim_hidden=50, dim_output=2)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "YnPAY1rnaozY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = weibull_loss()\n",
        "nb_epochs = 50"
      ],
      "metadata": {
        "id": "Bx_-qnBkao2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_loss_t, test_loss_t = fit(model, X_train, y_train, X_test, y_test, optimizer, loss_fn, nb_epochs, device)"
      ],
      "metadata": {
        "id": "4ZvZwqDlao7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(train_loss_t, test_loss_t):\n",
        "    nb_epochs = len(train_loss_t)\n",
        "    plt.plot(range(nb_epochs), train_loss_t, color='orange', label='Loss on the training set')\n",
        "    plt.plot(range(nb_epochs), test_loss_t, color='green', label='Loss on the testing set')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5PjqBU-BapAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_losses(train_loss_t, test_loss_t)"
      ],
      "metadata": {
        "id": "9vPMeJX7apEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_val = np.zeros(len(y_train))\n",
        "for i,y in enumerate(y_train):\n",
        "    max_val[i] = y[0].item()\n",
        "baseline = np.mean(max_val)"
      ],
      "metadata": {
        "id": "ULV9L2EeapID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_np(model, X_test, y_test, baseline=baseline, device=device, max_size=303):\n",
        "    n_test = len(X_test)\n",
        "    all_pred = np.empty((n_test,max_size,2))\n",
        "    all_y = np.empty((n_test,max_size))\n",
        "    base_pred = np.empty((n_test,max_size))\n",
        "    all_pred[:] = np.NaN\n",
        "    all_y[:] = np.NaN\n",
        "    base_pred[:] = np.NaN\n",
        "    list_npred = []\n",
        "    for k in range(n_test):\n",
        "        pred = model(X_test[k].to(device))\n",
        "        pred_np = pred.cpu().detach().numpy()\n",
        "        n_pred = pred_np.shape[0]\n",
        "        list_npred.append(n_pred)\n",
        "        all_pred[k,:n_pred,:] = pred_np\n",
        "        all_y[k,:n_pred] = y_test[k].numpy()\n",
        "        base_pred[k,:n_pred] = baseline - range(n_pred)\n",
        "    return all_pred, all_y, base_pred, list_npred"
      ],
      "metadata": {
        "id": "KGUNUiyMapLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_pred, all_y, base_pred, list_npred = compute_np(model, X_test, y_test)\n",
        "pred_fail = all_pred[:,:,0]*gamma(1+1/all_pred[:,:,1])"
      ],
      "metadata": {
        "id": "FLhKcuwgapOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 50\n",
        "plt.plot(pred_fail[k,:], label='predicted')\n",
        "plt.plot(all_y[k], label='true')\n",
        "plt.plot(base_pred[k], label='baseline')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "d5nec73rbJfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RMSE(pred_fail, all_y):\n",
        "    return np.sqrt((pred_fail-all_y)**2)"
      ],
      "metadata": {
        "id": "FpJvRp59bLlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res= RMSE(pred_fail,all_y)\n",
        "res_base = RMSE(base_pred, all_y)"
      ],
      "metadata": {
        "id": "VWdMtuRibLpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(res[k])\n",
        "plt.plot(res_base[k])"
      ],
      "metadata": {
        "id": "bX_f-0HbbLut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.nanmean(res,0), label='RMSE')\n",
        "plt.plot(np.nanmean(res_base,0), label='RMSE baseline')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "PYvL21babLxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.nanmean(res), np.nanmean(res_base)"
      ],
      "metadata": {
        "id": "5jaC65dJbL0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_indices = list((~np.isnan(res)).sum(axis = 1) - 1)"
      ],
      "metadata": {
        "id": "lRvplSlfbWBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean([res[i,j] for i,j in enumerate(last_indices)])"
      ],
      "metadata": {
        "id": "Q3Kh641tbWMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean([res_base[i,j] for i,j in enumerate(last_indices)])"
      ],
      "metadata": {
        "id": "OIZHCX7EbWRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot = sns.jointplot(x=[all_y[i,j] for i,j in enumerate(last_indices)],\n",
        "                     y=[base_pred[i,j] for i,j in enumerate(last_indices)],\n",
        "                     dropna=True, kind=\"kde\", n_levels=30, color=\"g\")\n",
        "plot.ax_joint.plot([0,150], [0,150], 'b-', linewidth = 2)\n",
        "plot.set_axis_labels('true', 'predicted')"
      ],
      "metadata": {
        "id": "-F0pB2D0bWXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot = sns.jointplot(x=[all_y[i,j] for i,j in enumerate(last_indices)],\n",
        "                     y=[pred_fail[i,j] for i,j in enumerate(last_indices)],\n",
        "                     dropna=True, kind=\"kde\", n_levels=30, color=\"g\")\n",
        "plot.ax_joint.plot([0,150], [0,150], 'b-', linewidth = 2)\n",
        "plot.set_axis_labels('true', 'predicted')"
      ],
      "metadata": {
        "id": "DHn11ILHbWfO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}