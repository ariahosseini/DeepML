{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsF+0L6fQcHAqJ22bI4D8B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariahosseini/DeepML/blob/main/034_PyTorch_Proj_ThirtyFour_MobileNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfgsxpHqbvgi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "__all__ = [\"MobileNetV2\", \"mobilenet_v2\"]\n",
        "\n",
        "class ConvBNReLU(nn.Sequential):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv2d(\n",
        "                in_planes,\n",
        "                out_planes,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                groups=groups,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_planes),\n",
        "            nn.ReLU6(inplace=True),\n",
        "        )\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        layers = []\n",
        "        if expand_ratio != 1:\n",
        "            # pw\n",
        "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
        "        layers.extend(\n",
        "            [\n",
        "                # dw\n",
        "                ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            ]\n",
        "        )\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(self, num_classes=10, width_mult=1.0):\n",
        "        super(MobileNetV2, self).__init__()\n",
        "        block = InvertedResidual\n",
        "        input_channel = 32\n",
        "        last_channel = 1280\n",
        "\n",
        "        # CIFAR10\n",
        "        inverted_residual_setting = [\n",
        "            # t, c, n, s\n",
        "            [1, 16, 1, 1],\n",
        "            [6, 24, 2, 1],  # Stride 2 -> 1 for CIFAR-10\n",
        "            [6, 32, 3, 2],\n",
        "            [6, 64, 4, 2],\n",
        "            [6, 96, 3, 1],\n",
        "            [6, 160, 3, 2],\n",
        "            [6, 320, 1, 1],\n",
        "        ]\n",
        "        # END\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = int(input_channel * width_mult)\n",
        "        self.last_channel = int(last_channel * max(1.0, width_mult))\n",
        "\n",
        "        # CIFAR10: stride 2 -> 1\n",
        "        features = [ConvBNReLU(3, input_channel, stride=1)]\n",
        "        # END\n",
        "\n",
        "        # building inverted residual blocks\n",
        "        for t, c, n, s in inverted_residual_setting:\n",
        "            output_channel = int(c * width_mult)\n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1\n",
        "                features.append(\n",
        "                    block(input_channel, output_channel, stride, expand_ratio=t)\n",
        "                )\n",
        "                input_channel = output_channel\n",
        "        # building last several layers\n",
        "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
        "        # make it nn.Sequential\n",
        "        self.features = nn.Sequential(*features)\n",
        "\n",
        "        # building classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(self.last_channel, num_classes),\n",
        "        )\n",
        "\n",
        "        # weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.mean([2, 3])\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def mobilenet_v2(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a MobileNetV2 architecture from\n",
        "    `\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" <https://arxiv.org/abs/1801.04381>`_.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    model = MobileNetV2(**kwargs)\n",
        "    if pretrained:\n",
        "        script_dir = os.path.dirname(__file__)\n",
        "        state_dict = torch.load(\n",
        "            script_dir + \"/state_dicts/mobilenet_v2.pt\", map_location=device\n",
        "        )\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model"
      ]
    }
  ]
}