{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP8CdJbZCDnQ/3KlL2Lbtkr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariahosseini/DeepML/blob/main/030_PyTorch_Proj_Thirty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utils\n",
        "import os, sys, collections, time\n",
        "import math, random\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "# sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "2CGo5Q916l8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec"
      ],
      "metadata": {
        "id": "fdl1dHinnK9i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QTxIndlexfq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da972e9b-b316-4912-dfe8-e39ee1db9b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n",
            "--2024-05-20 17:35:54--  https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5101618 (4.9M) [text/plain]\n",
            "Saving to: ‘ptb.train.txt’\n",
            "\n",
            "ptb.train.txt       100%[===================>]   4.87M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2024-05-20 17:35:55 (60.1 MB/s) - ‘ptb.train.txt’ saved [5101618/5101618]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "%cd data\n",
        "!wget https://raw.githubusercontent.com/tomsercu/lstm/master/data/ptb.train.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR='/content'\n",
        "data_path = os.path.join(ROOT_DIR,'data/')\n",
        "file = 'ptb.train.txt'\n",
        "with open(data_path+file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    raw_dataset = [st.split() for st in lines]\n",
        "'# sentences: %d' % len(raw_dataset)"
      ],
      "metadata": {
        "id": "a_mGXdnvm4Kn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "7a3d3a82-d22e-4f50-cd27-ae1efe637dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# sentences: 42068'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "KP_RKwE8nZrd",
        "outputId": "ed510c75-1cfa-4bb8-d511-90e5e121a61a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for st in raw_dataset[:3]:\n",
        "    print(len(st), st[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7xFAU6iE710",
        "outputId": "e92b20ad-d46f-4e03-d54e-e988f5f81240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24 ['aer', 'banknote', 'berlitz', 'calloway', 'centrust']\n",
            "15 ['pierre', '<unk>', 'N', 'years', 'old']\n",
            "11 ['mr.', '<unk>', 'is', 'chairman', 'of']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter = collections.Counter([tk for st in raw_dataset for tk in st])\n",
        "counter = dict(filter(lambda x: x[1] >= 5, counter.items()))"
      ],
      "metadata": {
        "id": "v36dY8El6pIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter['the'], counter['N'], counter['<unk>']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58c5qKqG6pS2",
        "outputId": "bf0f4f4a-2f74-48c1-9b72-b22201a0bf1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50770, 32481, 45020)"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_token = [tk for tk, _ in counter.items()]\n",
        "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n",
        "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n",
        "           for st in raw_dataset]\n",
        "num_tokens = sum([len(st) for st in dataset])\n",
        "'# tokens: %d' % num_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UM-2z-I66pYl",
        "outputId": "540ead26-289b-4d76-f551-f30d62443318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# tokens: 887100'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_token[:5], token_to_idx['consensus'], token_to_idx['pierre']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h40QO4zH6pdM",
        "outputId": "286f0290-8e0c-4dca-dc98-286691fd85e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['pierre', '<unk>', 'N', 'years', 'old'], 4827, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6rb8wr76pho",
        "outputId": "0afb4921-fa5b-4818-fe48-1054460c8e39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[],\n",
              " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 2],\n",
              " [14, 1, 15, 16, 17, 1, 18, 7, 19, 20, 21]]"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def discard(idx):\n",
        "    return random.uniform(0, 1) < 1 - math.sqrt(1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
        "\n",
        "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
        "'# tokens: %d' % sum([len(st) for st in subsampled_dataset])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "w0zLHTw3I8MM",
        "outputId": "7faa5f19-41bd-49c2-94a5-5ff457f8a681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# tokens: 375975'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_counts(token):\n",
        "    return '# of \"%s\": before=%d, after=%d' % (token, sum(\n",
        "        [st.count(token_to_idx[token]) for st in dataset]), sum(\n",
        "        [st.count(token_to_idx[token]) for st in subsampled_dataset]))\n",
        "\n",
        "compare_counts('the'), compare_counts('join')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPe5Arl_I8SN",
        "outputId": "33c8cf43-2c46-46d6-c4b3-d09816fd2bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('# of \"the\": before=50770, after=2121', '# of \"join\": before=45, after=45')"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_centers_and_contexts(dataset, max_window_size):\n",
        "    centers, contexts = [], []\n",
        "    for st in dataset:\n",
        "        if len(st) < 2:\n",
        "            continue\n",
        "        centers += st\n",
        "        for center_i in range(len(st)):\n",
        "            window_size = random.randint(1, max_window_size)\n",
        "            indices = list(range(max(0, center_i - window_size),\n",
        "                                 min(len(st), center_i + 1 + window_size)))\n",
        "            indices.remove(center_i)\n",
        "            contexts.append([st[idx] for idx in indices])\n",
        "    return centers, contexts"
      ],
      "metadata": {
        "id": "q-c1c1D6I8V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_dataset = [[0, 2, 1, 2, 3, 5, 4, 5, 6], list(range(7, 10))]\n",
        "print('dataset', tiny_dataset)\n",
        "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 3)):\n",
        "    print('center', center, 'has contexts', context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_r29pYfI8Zo",
        "outputId": "df9da1f0-e0bd-49a4-a593-db687a1ec8a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset [[0, 2, 1, 2, 3, 5, 4, 5, 6], [7, 8, 9]]\n",
            "center 0 has contexts [2, 1, 2]\n",
            "center 2 has contexts [0, 1, 2]\n",
            "center 1 has contexts [0, 2, 2, 3]\n",
            "center 2 has contexts [0, 2, 1, 3, 5, 4]\n",
            "center 3 has contexts [1, 2, 5, 4]\n",
            "center 5 has contexts [1, 2, 3, 4, 5, 6]\n",
            "center 4 has contexts [2, 3, 5, 5, 6]\n",
            "center 5 has contexts [5, 4, 6]\n",
            "center 6 has contexts [5, 4, 5]\n",
            "center 7 has contexts [8, 9]\n",
            "center 8 has contexts [7, 9]\n",
            "center 9 has contexts [7, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)"
      ],
      "metadata": {
        "id": "ViEUM6IyI8em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_negatives(all_contexts, sampling_weights, K):\n",
        "    all_negatives, neg_candidates, i = [], [], 0\n",
        "    population = list(range(len(sampling_weights)))\n",
        "    for contexts in all_contexts:\n",
        "        negatives = []\n",
        "        while len(negatives) < len(contexts) * K:\n",
        "            if i == len(neg_candidates):\n",
        "                # an index of k words is randomly generated as noise words based on the weight of each word (sampling_weights)\n",
        "                # for efficient calculation, k can be set slightly larger\n",
        "                i, neg_candidates = 0, random.choices(population, sampling_weights, k=int(1e5))\n",
        "            neg, i = neg_candidates[i], i + 1\n",
        "            if neg not in set(contexts): # noise words cannot be context words\n",
        "                negatives.append(neg)\n",
        "        all_negatives.append(negatives)\n",
        "    return all_negatives\n",
        "\n",
        "sampling_weights = [counter[w]**0.75 for w in idx_to_token]\n",
        "all_negatives = get_negatives(all_contexts, sampling_weights, 5)"
      ],
      "metadata": {
        "id": "CKhRGaEAI8jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_negatives[0], all_contexts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4I2BL_eI8mX",
        "outputId": "e27d3879-520c-422a-a4b9-da25e8e9792d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([84, 83, 108, 7972, 628, 1803, 264, 7554, 3889, 1366], [3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PTB_dataset(Dataset):\n",
        "\n",
        "    def __init__(self, all_centers, all_contexts, all_negatives):\n",
        "        self.all_centers, self.all_contexts_negatives, self.all_masks, self.all_labels = self.batchify(list(zip(all_centers,all_contexts,all_negatives)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_centers)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        return self.all_centers[idx], self.all_contexts_negatives[idx], self.all_masks[idx], self.all_labels[idx]\n",
        "\n",
        "    def batchify(self,data):\n",
        "        max_len = max(len(c) + len(n) for _, c, n in data)\n",
        "        centers, contexts_negatives, masks, labels = [], [], [], []\n",
        "        for center, context, negative in data:\n",
        "            cur_len = len(context) + len(negative)\n",
        "            centers += [center]\n",
        "            contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
        "            masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
        "            labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
        "        return (torch.tensor(centers).view((-1, 1)), torch.tensor(np.array(contexts_negatives)),\n",
        "            torch.tensor(np.array(masks)), torch.tensor(np.array(labels)))"
      ],
      "metadata": {
        "id": "LY_akt0pa-rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ptbdata = PTB_dataset(all_centers, all_contexts, all_negatives)\n",
        "ptbdata[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxGZ35R0bKS5",
        "outputId": "7e048e20-de9f-4adb-f2c7-96e0ecd021f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([3]),\n",
              " tensor([   0,    4,    5,    6,   11,   12,  885,  662,   14, 3469,    7, 1336,\n",
              "          376,  336, 6799,  353,  286,  519,  299, 1729,   74, 2155,  317, 5695,\n",
              "         2599,  226, 4261, 3099, 1908, 3891,  649, 6650, 5348, 6280, 4711, 1527,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]),\n",
              " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "data_loader = DataLoader(ptbdata, batch_size, shuffle=True, num_workers=4)\n",
        "for batch in data_loader:\n",
        "    for name, data in zip(['centers', 'contexts_negatives', 'masks', 'labels'], batch):\n",
        "        print(name, 'shape:', data.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZfXjf9PbKZq",
        "outputId": "1d1d459c-07ec-4a9e-c94f-58db2af44019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "centers shape: torch.Size([512, 1])\n",
            "contexts_negatives shape: torch.Size([512, 60])\n",
            "masks shape: torch.Size([512, 60])\n",
            "labels shape: torch.Size([512, 60])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledEmbedding(nn.Embedding): # Skip-Gram model\n",
        "    \"\"\"\n",
        "    Embedding layer that initialises its values\n",
        "    to using a normal variable scaled by the inverse\n",
        "    of the emedding dimension.\n",
        "    \"\"\"\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"\n",
        "        Initialize parameters.\n",
        "        \"\"\"\n",
        "        self.weight.data.normal_(0, 1.0 / self.embedding_dim)\n",
        "        if self.padding_idx is not None:\n",
        "            self.weight.data[self.padding_idx].fill_(0)\n",
        "\n",
        "class Skip_gram(nn.Module):\n",
        "    def __init__(self, input_dim, embed_size = 100):\n",
        "        super(Skip_gram, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.embed_size = embed_size\n",
        "        self.central_emb = ScaledEmbedding(self.input_dim,self.embed_size)\n",
        "        self.context_emb = ScaledEmbedding(self.input_dim,self.embed_size)\n",
        "\n",
        "    def forward(self, icent, icont):\n",
        "        cent_emb = self.central_emb(icent)\n",
        "        cont_emb = self.context_emb(icont)\n",
        "        return torch.einsum('bij,bkj -> bik' , cent_emb, cont_emb)"
      ],
      "metadata": {
        "id": "jIeAfU4qbKd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Skip_gram(len(idx_to_token))"
      ],
      "metadata": {
        "id": "Oi87Y6rNbKjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net(torch.tensor([0,1,3]).unsqueeze(1),torch.tensor([[0,0],[0,0],[0,0]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl_jiJ8tbKmP",
        "outputId": "eb1a8164-2ce6-42df-b4c3-73efe8456f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 5.8173e-05,  5.8173e-05]],\n",
              "\n",
              "        [[-8.0108e-04, -8.0108e-04]],\n",
              "\n",
              "        [[-1.4879e-04, -1.4879e-04]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
        "def criterion(pred, label, mask):\n",
        "    return (loss_fn(pred, label)*mask).sum(1)/mask.sum(1)"
      ],
      "metadata": {
        "id": "bd2JTyODbKss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = torch.tensor([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]])\n",
        "# 1 and 0 in the label variables label represent context words and the noise words, respectively\n",
        "label = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0]]).type(torch.FloatTensor)\n",
        "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 0]]).type(torch.FloatTensor)  # mask variable\n",
        "criterion(pred, label, mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ83uSpGbKw0",
        "outputId": "8cb11288-506a-452f-cc83-997204038c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.8740, 1.2100])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.005)"
      ],
      "metadata": {
        "id": "4stO4K1RdQIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "SfrI8SYRdjc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(n_epochs):\n",
        "    for epoch in range(n_epochs):\n",
        "        start, loss = time.time(), 0\n",
        "        for batch in data_loader:\n",
        "            cent, cont, mas, lab = batch\n",
        "            cent = cent.to(device)\n",
        "            cont = cont.to(device)\n",
        "            mas = mas.to(device)\n",
        "            lab = lab.type(torch.FloatTensor).to(device)\n",
        "            pred = net(cent,cont).squeeze()\n",
        "            optimizer.zero_grad()\n",
        "            curr_loss = criterion(pred,lab,mas).mean()\n",
        "            curr_loss.backward()\n",
        "            optimizer.step()\n",
        "            loss += curr_loss.item()\n",
        "        print('epoch %d, loss %.2f, time %.2fs'\n",
        "              % (epoch + 1, loss, time.time() - start))"
      ],
      "metadata": {
        "id": "3ocQ5MVAdQNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR-w_QHAdQRl",
        "outputId": "57a62917-2461-4790-ec61-b7d14167585d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, loss 334.34, time 40.09s\n",
            "epoch 2, loss 290.13, time 33.05s\n",
            "epoch 3, loss 259.21, time 33.23s\n",
            "epoch 4, loss 237.44, time 35.43s\n",
            "epoch 5, loss 224.14, time 35.12s\n",
            "epoch 6, loss 215.36, time 34.01s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_tokens(query_token, k, W):\n",
        "    x = W[token_to_idx[query_token]]\n",
        "    cos = torch.matmul(W,x) / torch.sqrt(torch.sum(W*W,1)*torch.sum(x*x)+1e-9)\n",
        "    _,topk = torch.topk(cos, k=k+1,)\n",
        "    for i in topk[1:]: # remove the input words\n",
        "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n",
        "get_similar_tokens('chip', 3, net.central_emb.weight.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvoIeWP5dQUP",
        "outputId": "5ac6521d-6842-49c7-8fff-c25284638d20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine sim=0.608: intel\n",
            "cosine sim=0.528: chips\n",
            "cosine sim=0.499: workstation\n"
          ]
        }
      ]
    }
  ]
}