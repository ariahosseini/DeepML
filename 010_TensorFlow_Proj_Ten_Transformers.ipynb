{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyhmfkLWlZR+XbjXRWyujv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariahosseini/DeepML/blob/main/010_TensorFlow_Proj_Ten_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qK7ubjymz2C"
      },
      "outputs": [],
      "source": [
        "# Libraries:\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional embedding:\n",
        "class TokenAndPositionEmbedding(tf.keras.layers.Layer): # Define a new class TokenAndPositionEmbedding that inherits from the tf.keras.layers.Layer base class.\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim): # The constructor takes three parameters:\n",
        "        # maxlen, vocab_size, and embed_dim\n",
        "        # super() is used to call the constructor of the base class, which is tf.keras.layers.Layer in this case.\n",
        "        # This is done to ensure that the initialization code for the base class is executed before the derived class's own initialization code.\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        # Instantiate an embedding layer for tokens, with vocab_size input dimensions and embed_dim output dimensions. You can define your own custom embedding as well.\n",
        "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        # Instantiate another embedding layer for positional information with maxlen input dimensions and embed_dim output dimensions.\n",
        "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x): # It takes one argument, x (input tensor).\n",
        "        maxlen = tf.shape(x)[-1] # Obtain the maximum sequence length by calling tf.shape(x)[-1]. In our example = 200\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1) # This range represents the positions of the tokens in the input sequence.\n",
        "        positions = self.pos_emb(positions) # Obtain the positional embeddings by passing the positions through the position embedding layer (self.pos_emb)\n",
        "        x = self.token_emb(x) # Obtain the token embeddings by passing the input tensor x through the token embedding layer (self.token_emb).\n",
        "        return x + positions # Add the positional embeddings to the token embeddings, element-wise, and return the result. (32 ,200, 128)"
      ],
      "metadata": {
        "id": "JqH_z0Q5nLbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformer layer:\n",
        "class MultiHeadSelfAttention(tf.keras.layers.Layer): # Define a new class MultiHeadSelfAttention that inherits from the tf.keras.layers.Layer base class.\n",
        "    def __init__(self, embed_dim, num_heads=8): # The constructor for the class takes two parameters, embed_dim and num_heads.\n",
        "        #  super() is used to call the constructor of the base class, which is tf.keras.layers.Layer in this case.\n",
        "        # This is done to ensure that the initialization code for the base class is executed before the derived class's own initialization code.\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim # 128\n",
        "        self.num_heads = num_heads # 2\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "\n",
        "        # projection_dim, is the dimension of each projected head in the multi-head self-attention mechanism\n",
        "        self.projection_dim = embed_dim // num_heads # 64\n",
        "        # The projection_dim is used to divide the input embeddings into separate parts for each attention head.\n",
        "        # In the multi-head self-attention mechanism, the input embeddings are split into multiple smaller parts,\n",
        "        # allowing the model to focus on different aspects of the input and capture a variety of patterns.\n",
        "\n",
        "        # The followings are dense (fully connected) layers responsible for computing the query, key, and value matrices, respectively, from the input embeddings.\n",
        "        self.query_dense = tf.keras.layers.Dense(embed_dim)\n",
        "        self.key_dense = tf.keras.layers.Dense(embed_dim)\n",
        "        self.value_dense = tf.keras.layers.Dense(embed_dim)\n",
        "\n",
        "        # Another dense layer that combines the outputs from all attention heads.\n",
        "        self.combine_heads = tf.keras.layers.Dense(embed_dim)\n",
        "\n",
        "    # This method calculates the attention scores, scales them, applies the softmax function to obtain the attention weights,\n",
        "    # and then computes the output by multiplying the attention weights with the value matrix.\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True) # (32, 200, 128) * (32, 128, 200) = (32, 200, 200)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) # tf.cast is a TensorFlow function used to change the data type of a tensor.  #  128\n",
        "        # By using [-1], you are selecting the last element of the shape tensor. In this case, it corresponds to the dimension of the key vectors.\n",
        "\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)   # (32, 200, 200)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1) # (32, 200, 200)\n",
        "        output = tf.matmul(weights, value)             # (32, 200, 200) * (32, 200, 128) = (32, 200, 128)\n",
        "        return output, weights\n",
        "\n",
        "    # This method is used to separate the different heads in the multi-head self-attention mechanism.\n",
        "    # It reshapes and transposes the input tensor according to the number of attention heads.\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) # (batch_size, seq_length, self.num_heads, self.projection_dim) = (32, 200, 2, 64)\n",
        "        # By using -1, you're telling TensorFlow to compute the sequence length based on the input tensor's total number of elements,\n",
        "        # divided by the product of batch_size, self.num_heads, and self.projection_dim.\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3]) #  The perm argument specifies the new order of the axes. (batch_size, self.num_heads, seq_length, self.projection_dim) = (32, 2, 200, 64)\n",
        "\n",
        "    # This method is the main entry point of the layer, and it is called when the layer is used in the model.\n",
        "    # It takes the input tensor, computes the query, key, and value matrices, separates the heads,\n",
        "    # calculates the attention and attention weights, combines the heads, and returns the final output.\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]               # 32\n",
        "        query = self.query_dense(inputs)               # (32, 200, 128)\n",
        "        key = self.key_dense(inputs)                   # (32, 200, 128)\n",
        "        value = self.value_dense(inputs)               # (32, 200, 128)\n",
        "        query = self.separate_heads(query, batch_size) # (32, 2, 200, 64)\n",
        "        key = self.separate_heads(key, batch_size)     # (32, 2, 200, 64)\n",
        "        value = self.separate_heads(value, batch_size) # (32, 2, 200, 64)\n",
        "        attention, weights = self.attention(query, key, value) # attention (32, 200, 2, 64)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) # attention (32, 2, 200, 64)\n",
        "\n",
        "        # After the attention computation, the outputs from all heads are combined back into a single vector of the original embedding dimension.\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) # (32, 200, 128)\n",
        "        output = self.combine_heads(concat_attention) # dense layer that combines the outputs from all attention heads.\n",
        "        return output"
      ],
      "metadata": {
        "id": "2EjqYHgCnLeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Transformer block:\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): # The constructor takes four parameters:\n",
        "        # embed_dim, num_heads, ff_dim, and rate.\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Instantiate the MultiHeadSelfAttention layer using the provided embed_dim and num_heads parameters.\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "\n",
        "        # Define the position-wise feed-forward network (FFN) as a sequential model with two dense layers.\n",
        "        # The first dense layer has ff_dim units and a ReLU activation function, while the second dense layer has embed_dim units.\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "\n",
        "        # Instantiate two layer normalization layers with an epsilon value of 1e-6 to stabilize the layer normalization process.\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Instantiate two dropout layers with the given dropout rate rate.\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "    # This method is the main entry point of the layer, and it is called when the layer is used in the model.\n",
        "    # It takes two arguments: inputs (input tensor) and training (a boolean indicating whether the model is in training mode).\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs) # Pass the input through the multi-head self-attention layer (self.att).\n",
        "        attn_output = self.dropout1(attn_output, training=training) # Apply the first dropout layer (self.dropout1) to the attention output.\n",
        "        out1 = self.layernorm1(inputs + attn_output) # Add the attention output to the original input (residual connection) and apply the first layer normalization (self.layernorm1).\n",
        "        ffn_output = self.ffn(out1) # Pass the output through the feed-forward network (self.ffn).\n",
        "        ffn_output = self.dropout2(ffn_output, training=training) # Apply the second dropout layer (self.dropout2) to the feed-forward network's output.\n",
        "        return self.layernorm2(out1 + ffn_output) # Add the feed-forward network's output to the output from previous step (out1) (another residual connection),\n",
        "                                                  # and apply the second layer normalization (self.layernorm2)."
      ],
      "metadata": {
        "id": "buN6mf42nLhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "def create_transformer_model(maxlen, vocab_size, embed_dim, num_heads, ff_dim, num_blocks, num_classes, dropout_rate=0.1):\n",
        "    inputs = tf.keras.layers.Input(shape=(maxlen,))\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "\n",
        "    # Loop num_blocks times to create a stacked transformer architecture with the specified number of blocks.\n",
        "    for _ in range(num_blocks):\n",
        "        x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)(x)\n",
        "\n",
        "    # After passing through all the transformer blocks, apply global average pooling to reduce the tensor's dimensions and capture the most relevant features.\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Add a fully connected dense layer with 30 hidden units and a ReLU activation function for further feature extraction.\n",
        "    x = tf.keras.layers.Dense(30, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "VPm-15qcnLkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and prep the data\n",
        "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haQibikpnLnh",
        "outputId": "eaf3ad30-e9ea-4247-9887-146de5fa6eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "vocab_size = 20000 # Consider the top 20k words (size of the vocabulary).\n",
        "maxlen = 200 # Consider the first 200 words of each movie review (maximum length of input sequences).\n",
        "embed_dim = 128 # Dimension of the embeddings.\n",
        "num_heads = 2 # Number of attention heads in the multi-head self-attention mechanism.\n",
        "ff_dim = 30 # Eimension of the feed-forward network's hidden layer.\n",
        "num_blocks = 2\n",
        "num_classes = 2\n",
        "dropout_rate = 0.1 # Dropout rate\n",
        "\n",
        "model = create_transformer_model(maxlen, vocab_size, embed_dim, num_heads, ff_dim, num_blocks, num_classes, dropout_rate)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=1, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0S1z5oSnLqa",
        "outputId": "b3a9204a-5df4-45cd-d0c2-898aa01616a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 466s 590ms/step - loss: 0.3759 - accuracy: 0.8224 - val_loss: 0.3052 - val_accuracy: 0.8662\n"
          ]
        }
      ]
    }
  ]
}