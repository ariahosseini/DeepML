{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "1O2nJalPJGZM",
        "r3saEfpuKr8S",
        "nVDaZg5B0LK9"
      ],
      "authorship_tag": "ABX9TyMJxo5tbIQe3NWXRxCvM1RI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariahosseini/DeepML/blob/main/016_TensorFlow_Proj_Sixteen_VariationalAutoEncoder_GNN_Spektral_EdgeLevel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link to the doc: https://arxiv.org/pdf/1611.07308.pdf"
      ],
      "metadata": {
        "id": "A1-DDkCxI3iW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rvusxE0zEGS"
      },
      "outputs": [],
      "source": [
        "# install\n",
        "# !pip install spektral"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "# sklearn\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "# tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "# spektral\n",
        "from spektral.datasets import citation\n",
        "from spektral.layers import GCNConv\n",
        "from spektral.utils.sparse import sp_matrix_to_sp_tensor"
      ],
      "metadata": {
        "id": "dMekWgjdJLQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
        "# if len(physical_devices) > 0:\n",
        "#     tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "metadata": {
        "id": "GN35w4-75eye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "1O2nJalPJGZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "    if not sp.isspmatrix_coo(sparse_mx):\n",
        "        sparse_mx = sparse_mx.tocoo()\n",
        "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
        "    values = sparse_mx.data\n",
        "    shape = sparse_mx.shape\n",
        "    return coords, values, shape"
      ],
      "metadata": {
        "id": "TKT9WvidJG7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_test_edges(adj): # func to build test set with 10% positive links\n",
        "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape) # remove diagonal elements\n",
        "    adj.eliminate_zeros()\n",
        "    assert np.diag(adj.todense()).sum() == 0 # check that diag is zero\n",
        "    adj_triu = sp.triu(adj)\n",
        "    adj_tuple = sparse_to_tuple(adj_triu)\n",
        "    edges = adj_tuple[0]\n",
        "    edges_all = sparse_to_tuple(adj)[0]\n",
        "    num_test = int(np.floor(edges.shape[0] / 10.))\n",
        "    num_val = int(np.floor(edges.shape[0] / 20.))\n",
        "    all_edge_idx = list(range(edges.shape[0]))\n",
        "    np.random.shuffle(all_edge_idx)\n",
        "    val_edge_idx = all_edge_idx[:num_val]\n",
        "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
        "    test_edges = edges[test_edge_idx]\n",
        "    val_edges = edges[val_edge_idx]\n",
        "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
        "    def ismember(a, b, tol=5):\n",
        "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
        "        return np.any(rows_close)\n",
        "    test_edges_false = []\n",
        "    while len(test_edges_false) < len(test_edges):\n",
        "        idx_i = np.random.randint(0, adj.shape[0])\n",
        "        idx_j = np.random.randint(0, adj.shape[0])\n",
        "        if idx_i == idx_j:\n",
        "            continue\n",
        "        if ismember([idx_i, idx_j], edges_all):\n",
        "            continue\n",
        "        if test_edges_false:\n",
        "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
        "                continue\n",
        "        test_edges_false.append([idx_i, idx_j])\n",
        "    val_edges_false = []\n",
        "    while len(val_edges_false) < len(val_edges):\n",
        "        idx_i = np.random.randint(0, adj.shape[0])\n",
        "        idx_j = np.random.randint(0, adj.shape[0])\n",
        "        if idx_i == idx_j:\n",
        "            continue\n",
        "        if ismember([idx_i, idx_j], train_edges):\n",
        "            continue\n",
        "        if ismember([idx_j, idx_i], train_edges):\n",
        "            continue\n",
        "        if ismember([idx_i, idx_j], val_edges):\n",
        "            continue\n",
        "        if ismember([idx_j, idx_i], val_edges):\n",
        "            continue\n",
        "        if val_edges_false:\n",
        "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
        "                continue\n",
        "        val_edges_false.append([idx_i, idx_j])\n",
        "    assert ~ismember(test_edges_false, edges_all)\n",
        "    assert ~ismember(val_edges_false, edges_all)\n",
        "    assert ~ismember(val_edges, train_edges)\n",
        "    assert ~ismember(test_edges, train_edges)\n",
        "    assert ~ismember(val_edges, test_edges)\n",
        "    data = np.ones(train_edges.shape[0])\n",
        "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape) # re-build adj matrix\n",
        "    adj_train = adj_train + adj_train.T\n",
        "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false # note: these edge lists only contain single direction of edge!"
      ],
      "metadata": {
        "id": "TKH7bj_FJHpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_roc_score(edges_pos, edges_neg, adj_rec):\n",
        "    preds = []\n",
        "    for e in edges_pos:\n",
        "        preds.append(adj_rec[e[0], e[1]])\n",
        "    preds_neg = []\n",
        "    for e in edges_neg:\n",
        "        preds_neg.append(adj_rec[e[0], e[1]])\n",
        "    preds_all = np.hstack([preds, preds_neg])\n",
        "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
        "    roc_score = roc_auc_score(labels_all, preds_all)\n",
        "    ap_score = average_precision_score(labels_all, preds_all)\n",
        "    return roc_score, ap_score"
      ],
      "metadata": {
        "id": "bT4qevx5JHsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoEncoder"
      ],
      "metadata": {
        "id": "r3saEfpuKr8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "dataset = citation.Cora()\n",
        "graph = dataset[0]\n",
        "nodes = graph.x # nodes features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbWO-czXJHvn",
        "outputId": "8bc1e0d2-884b-46a8-cfec-341e36c93baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading cora dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/sparse/_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# target graph to reconstruct\n",
        "adj_label = graph.a + sp.eye(graph.a.shape[0], dtype=np.float32)\n",
        "adj_label = adj_label.toarray().reshape([-1])"
      ],
      "metadata": {
        "id": "R-rqv89PKwIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove edges randomly from training set and put them in the validation/test sets\n",
        "adj_train, _, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(graph.a)"
      ],
      "metadata": {
        "id": "7N1loKwaVD8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the adj matrix and convert it to sparse tensor\n",
        "adj_norm = GCNConv.preprocess(adj_train)\n",
        "adj_norm = sp_matrix_to_sp_tensor(adj_norm)"
      ],
      "metadata": {
        "id": "_oyznCWmVEAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the class weights (necessary due to imbalanceness in the number of non-zero edges)\n",
        "pos_weight = float(adj_train.shape[0] * adj_train.shape[0] - adj_train.sum()) / adj_train.sum()"
      ],
      "metadata": {
        "id": "VAkBaFcOVEDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# params\n",
        "hidden_dim1, hidden_dim2 = 32, 16 # units in the GCN layers\n",
        "dropout = 0.0                     # dropout rate\n",
        "l2_reg = 0e-5                     # L2 regularization rate\n",
        "learning_rate = 1e-2              # learning rate\n",
        "epochs = 20                      # max number of training epochs\n",
        "val_epochs = 2                   # after how many epochs should check the validation set\n",
        "num_nodes = dataset.n_nodes               # number of nodes in the graph\n",
        "num_feats = dataset.n_node_features       # original size of node features"
      ],
      "metadata": {
        "id": "ZRzw-JGHVOm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GNN architecture\n",
        "nodes_input = Input(shape=(num_feats,))\n",
        "adj_input = Input((num_nodes,), sparse=True)\n",
        "gc = GCNConv(hidden_dim1, activation=\"relu\", kernel_regularizer=l2(l2_reg))([nodes_input, adj_input])\n",
        "gc = Dropout(dropout)(gc)\n",
        "z = GCNConv(hidden_dim2, activation=None, kernel_regularizer=l2(l2_reg))([gc, adj_input])\n",
        "output = tf.matmul(z, tf.transpose(z))\n",
        "adj_rec = tf.keras.layers.Activation('sigmoid')(output)\n",
        "output = tf.reshape(output, [-1])\n",
        "model = Model(inputs=[nodes_input, adj_input], outputs=[output, adj_rec, z]) # build model\n",
        "optimizer = Adam(learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "JBoErmM6VR5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train & test\n",
        "@tf.function\n",
        "def train():\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _, _ = model([nodes, adj_norm], training=True)\n",
        "        loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=predictions, labels=adj_label, pos_weight=pos_weight))\n",
        "        loss += sum(model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "# train\n",
        "best_val_roc = 0\n",
        "for epoch in range(1, epochs):\n",
        "    loss = train()\n",
        "    print(f\"epoch: {epoch:d} -- loss: {loss:.3f}\")\n",
        "    if epoch%val_epochs==0:\n",
        "        _, adj_rec, _ = model([nodes, adj_norm])\n",
        "        adj_rec = adj_rec.numpy()\n",
        "        val_roc, _ = get_roc_score(val_edges, val_edges_false, adj_rec)\n",
        "        if val_roc <= best_val_roc:\n",
        "            break\n",
        "        else:\n",
        "            best_val_roc = val_roc\n",
        "            acc = np.mean(np.round(adj_rec) == graph.a.toarray())\n",
        "            print(f\"Val AUC: {best_val_roc*100:.1f}, Accuracy: {acc*100:.1f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zsxi1fEWKwSX",
        "outputId": "4e8f5bf1-eff6-4f58-a909-28c9de64f376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 -- loss: 0.869\n",
            "epoch: 2 -- loss: 0.869\n",
            "Val AUC: 98.2, Accuracy: 57.4\n",
            "epoch: 3 -- loss: 0.868\n",
            "epoch: 4 -- loss: 0.867\n",
            "Val AUC: 98.3, Accuracy: 57.5\n",
            "epoch: 5 -- loss: 0.866\n",
            "epoch: 6 -- loss: 0.865\n",
            "Val AUC: 98.4, Accuracy: 57.5\n",
            "epoch: 7 -- loss: 0.865\n",
            "epoch: 8 -- loss: 0.864\n",
            "Val AUC: 98.5, Accuracy: 57.5\n",
            "epoch: 9 -- loss: 0.863\n",
            "epoch: 10 -- loss: 0.863\n",
            "Val AUC: 98.5, Accuracy: 57.5\n",
            "epoch: 11 -- loss: 0.862\n",
            "epoch: 12 -- loss: 0.861\n",
            "Val AUC: 98.6, Accuracy: 57.5\n",
            "epoch: 13 -- loss: 0.861\n",
            "epoch: 14 -- loss: 0.860\n",
            "Val AUC: 98.6, Accuracy: 57.5\n",
            "epoch: 15 -- loss: 0.860\n",
            "epoch: 16 -- loss: 0.859\n",
            "Val AUC: 98.6, Accuracy: 57.5\n",
            "epoch: 17 -- loss: 0.859\n",
            "epoch: 18 -- loss: 0.858\n",
            "Val AUC: 98.7, Accuracy: 57.5\n",
            "epoch: 19 -- loss: 0.858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "_, adj_rec, node_emb = model([nodes, adj_norm])\n",
        "adj_rec = adj_rec.numpy()\n",
        "roc_score, ap_score = get_roc_score(test_edges, test_edges_false, adj_rec)\n",
        "print(f\"AUC: {roc_score*100:.1f}, AP: {ap_score*100:.1f}\")\n",
        "test_acc = np.mean(np.round(adj_rec.ravel()) == adj_label)\n",
        "print(f\"Test accuracy: {test_acc*100:.1f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4TNmIOiJHyc",
        "outputId": "bfa3aa09-abd5-4169-f31b-91286e5c8eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 98.3, AP: 97.9\n",
            "Test accuracy: 57.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoEncoder with Wasserstein Loss"
      ],
      "metadata": {
        "id": "nVDaZg5B0LK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, Reshape, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "vjVKcjMp0NM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "def build_encoder(input_shape, latent_dim):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(32, kernel_size=3, strides=2, activation='relu', padding='same')(inputs)\n",
        "    x = Conv2D(64, kernel_size=3, strides=2, activation='relu', padding='same')(x)\n",
        "    x = Flatten()(x)\n",
        "    z = Dense(latent_dim, activation=None)(x)  # Latent space\n",
        "    return Model(inputs, z, name=\"encoder\")\n",
        "\n",
        "# Decoder\n",
        "def build_decoder(output_shape, latent_dim):\n",
        "    latent_inputs = Input(shape=(latent_dim,))\n",
        "    x = Dense(units=7*7*64, activation='relu')(latent_inputs)  # Adjust size as per your data\n",
        "    x = Reshape((7, 7, 64))(x)\n",
        "    x = Conv2DTranspose(64, kernel_size=3, strides=2, activation='relu', padding='same')(x)\n",
        "    x = Conv2DTranspose(32, kernel_size=3, strides=2, activation='relu', padding='same')(x)\n",
        "    outputs = Conv2DTranspose(output_shape[-1], kernel_size=3, activation='sigmoid', padding='same')(x)\n",
        "    return Model(latent_inputs, outputs, name=\"decoder\")\n"
      ],
      "metadata": {
        "id": "9OVXfEHp0NP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_critic(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(32, kernel_size=3, strides=2, activation='relu', padding='same')(inputs)\n",
        "    x = Conv2D(64, kernel_size=3, strides=2, activation='relu', padding='same')(x)\n",
        "    x = Flatten()(x)\n",
        "    outputs = Dense(1, activation=None)(x)  # Output score\n",
        "    return Model(inputs, outputs, name=\"critic\")\n"
      ],
      "metadata": {
        "id": "MnTTQaxA0NTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(y_true * y_pred)\n"
      ],
      "metadata": {
        "id": "Db2tAYDI0NXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_penalty(critic, real_samples, fake_samples):\n",
        "    alpha = tf.random.uniform(shape=[real_samples.shape[0], 1, 1, 1], minval=0.0, maxval=1.0)\n",
        "    interpolates = alpha * real_samples + (1 - alpha) * fake_samples\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolates)\n",
        "        predictions = critic(interpolates)\n",
        "\n",
        "    gradients = tape.gradient(predictions, interpolates)\n",
        "    gradient_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
        "    return tf.reduce_mean((gradient_norm - 1.0) ** 2)\n"
      ],
      "metadata": {
        "id": "ZN60Uttq0TPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(encoder, decoder, critic, optimizer_ae, optimizer_critic, real_samples, lambda_gp=10):\n",
        "    # Autoencoder forward pass\n",
        "    with tf.GradientTape() as tape_ae:\n",
        "        z = encoder(real_samples)\n",
        "        generated_samples = decoder(z)\n",
        "        ae_loss = tf.reduce_mean(tf.square(real_samples - generated_samples))\n",
        "\n",
        "    # Update autoencoder\n",
        "    gradients_ae = tape_ae.gradient(ae_loss, encoder.trainable_variables + decoder.trainable_variables)\n",
        "    optimizer_ae.apply_gradients(zip(gradients_ae, encoder.trainable_variables + decoder.trainable_variables))\n",
        "\n",
        "    # Critic forward pass\n",
        "    with tf.GradientTape() as tape_critic:\n",
        "        fake_scores = critic(generated_samples)\n",
        "        real_scores = critic(real_samples)\n",
        "        gp = gradient_penalty(critic, real_samples, generated_samples)\n",
        "        critic_loss = tf.reduce_mean(fake_scores) - tf.reduce_mean(real_scores) + lambda_gp * gp\n",
        "\n",
        "    # Update critic\n",
        "    gradients_critic = tape_critic.gradient(critic_loss, critic.trainable_variables)\n",
        "    optimizer_critic.apply_gradients(zip(gradients_critic, critic.trainable_variables))\n",
        "\n",
        "    return ae_loss, critic_loss\n"
      ],
      "metadata": {
        "id": "y85WhuR20TSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "latent_dim = 16\n",
        "input_shape = (28, 28, 1)  # Example for grayscale images\n",
        "output_shape = input_shape\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "lambda_gp = 10\n",
        "\n",
        "# Data\n",
        "(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "\n",
        "# Models\n",
        "encoder = build_encoder(input_shape, latent_dim)\n",
        "decoder = build_decoder(output_shape, latent_dim)\n",
        "critic = build_critic(output_shape)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_ae = Adam(learning_rate=0.0001, beta_1=0.5)\n",
        "optimizer_critic = Adam(learning_rate=0.0001, beta_1=0.5)\n",
        "\n",
        "# Training\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        real_samples = x_train[i:i + batch_size]\n",
        "        ae_loss, critic_loss = train_step(encoder, decoder, critic, optimizer_ae, optimizer_critic, real_samples, lambda_gp)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, AE Loss: {ae_loss.numpy()}, Critic Loss: {critic_loss.numpy()}\")\n"
      ],
      "metadata": {
        "id": "uTv4Ksh30TWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples(decoder, num_samples=10, latent_dim=16):\n",
        "    z = np.random.normal(size=(num_samples, latent_dim))\n",
        "    generated_samples = decoder.predict(z)\n",
        "    return generated_samples\n"
      ],
      "metadata": {
        "id": "xiAahP3Z0Na0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variational Auto Encoder"
      ],
      "metadata": {
        "id": "FjhCkQXqwdvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "dataset = citation.Cora()\n",
        "graph = dataset[0]\n",
        "nodes = graph.x # nodes features"
      ],
      "metadata": {
        "id": "R1ugRTEz1UY1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d621256-e512-4426-e74d-65d1a17e2544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/sparse/_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# target graph to reconstruct\n",
        "adj_label = graph.a + sp.eye(graph.a.shape[0], dtype=np.float32)\n",
        "adj_label = adj_label.toarray().reshape([-1])"
      ],
      "metadata": {
        "id": "jY9HGK6sY7bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove edges randomly from training set and put them in the validation/test sets\n",
        "adj_train, _, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(graph.a)"
      ],
      "metadata": {
        "id": "WSA1b2flY7lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the adj matrix and convert it to sparse tensor\n",
        "adj_norm = GCNConv.preprocess(adj_train)\n",
        "adj_norm = sp_matrix_to_sp_tensor(adj_norm)"
      ],
      "metadata": {
        "id": "cvgs3q0OZDFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the class weights (necessary due to imbalanceness in the number of non-zero edges)\n",
        "pos_weight = float(adj_train.shape[0] * adj_train.shape[0] - adj_train.sum()) / adj_train.sum()\n",
        "norm = adj_train.shape[0] * adj_train.shape[0] / float((adj_train.shape[0] * adj_train.shape[0] - adj_train.sum()) * 2)"
      ],
      "metadata": {
        "id": "m12Lg0mdZDOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# params\n",
        "hidden_dim1, hidden_dim2 = 32, 16 # units in the GCN layers\n",
        "dropout = 0.0                     # dropout rate\n",
        "l2_reg = 0e-5                     # L2 regularization rate\n",
        "learning_rate = 1e-2              # learning rate\n",
        "epochs = 20                       # max number of training epochs\n",
        "val_epochs = 2                    # after how many epochs should check the validation set\n",
        "num_nodes = dataset.n_nodes               # number of nodes in the graph\n",
        "num_feats = dataset.n_node_features       # original size of node features"
      ],
      "metadata": {
        "id": "WUpuBqMAZPyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GNN architecture\n",
        "nodes_input = Input(shape=(num_feats,))\n",
        "adj_input = Input((num_nodes,), sparse=True)\n",
        "gc = GCNConv(hidden_dim1, activation=\"relu\", kernel_regularizer=l2(l2_reg))([nodes_input, adj_input])\n",
        "gc = Dropout(dropout)(gc)\n",
        "z_mean = GCNConv(hidden_dim2, activation=None, kernel_regularizer=l2(l2_reg))([gc, adj_input])\n",
        "z_log_std = GCNConv(hidden_dim2, activation=None, kernel_regularizer=l2(l2_reg))([gc, adj_input])\n",
        "z = z_mean + tf.random.normal([num_nodes, hidden_dim2]) * tf.exp(z_log_std)\n",
        "output = tf.matmul(z, tf.transpose(z))\n",
        "output = tf.reshape(output, [-1])\n",
        "output_det = tf.matmul(z_mean, tf.transpose(z_mean)) # this is not used for training and we make it deterministic\n",
        "adj_rec = tf.keras.layers.Activation('sigmoid')(output_det)\n",
        "model = Model(inputs=[nodes_input, adj_input], outputs=[output, adj_rec, z_mean, z_log_std]) # build model\n",
        "optimizer = Adam(learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "eP11i41pZP7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tain\n",
        "@tf.function\n",
        "def train():\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _, model_z_mean, model_z_log_std = model([nodes, adj_norm], training=True)\n",
        "        rec_loss = norm*tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=predictions, labels=adj_label, pos_weight=pos_weight)) # reconstruction loss\n",
        "        kl_loss = (0.5 / num_nodes) * tf.reduce_mean(tf.reduce_sum(1 + 2 * model_z_log_std - tf.square(model_z_mean) - tf.square(tf.exp(model_z_log_std)), 1)) # latent loss\n",
        "        loss = rec_loss - kl_loss + sum(model.losses) # total loss\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "best_val_roc = 0\n",
        "for epoch in range(1, epochs):\n",
        "    loss = train()\n",
        "    print(f\"epoch: {epoch:d} -- loss: {loss:.3f}\")\n",
        "    if epoch%val_epochs==0:\n",
        "        _, adj_rec, _, _ = model([nodes, adj_norm])\n",
        "        adj_rec = adj_rec.numpy()\n",
        "        val_roc, _ = get_roc_score(val_edges, val_edges_false, adj_rec)\n",
        "        if val_roc <= best_val_roc:\n",
        "            break\n",
        "        else:\n",
        "            best_val_roc = val_roc\n",
        "            acc = np.mean(np.round(adj_rec) == graph.a.toarray())\n",
        "            print(f\"Val AUC: {best_val_roc*100:.1f}, Accuracy: {acc*100:.1f}\")"
      ],
      "metadata": {
        "id": "dws1aPLc1UK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbd8f8ba-1558-4046-d741-0dba076bf315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 -- loss: 1.863\n",
            "epoch: 2 -- loss: 1.452\n",
            "Val AUC: 70.5, Accuracy: 0.1\n",
            "epoch: 3 -- loss: 1.249\n",
            "epoch: 4 -- loss: 1.092\n",
            "Val AUC: 72.4, Accuracy: 0.1\n",
            "epoch: 5 -- loss: 0.960\n",
            "epoch: 6 -- loss: 0.863\n",
            "Val AUC: 76.9, Accuracy: 0.5\n",
            "epoch: 7 -- loss: 0.795\n",
            "epoch: 8 -- loss: 0.752\n",
            "Val AUC: 79.2, Accuracy: 2.4\n",
            "epoch: 9 -- loss: 0.725\n",
            "epoch: 10 -- loss: 0.704\n",
            "Val AUC: 83.8, Accuracy: 11.4\n",
            "epoch: 11 -- loss: 0.679\n",
            "epoch: 12 -- loss: 0.654\n",
            "Val AUC: 84.6, Accuracy: 33.9\n",
            "epoch: 13 -- loss: 0.627\n",
            "epoch: 14 -- loss: 0.597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "_, adj_rec, node_emb, _ = model([nodes, adj_norm])\n",
        "adj_rec = adj_rec.numpy()\n",
        "roc_score, ap_score = get_roc_score(test_edges, test_edges_false, adj_rec)\n",
        "print(f\"AUC: {roc_score*100:.1f}, AP: {ap_score*100:.1f}\")\n",
        "test_acc = np.mean(np.round(adj_rec.ravel()) == adj_label)\n",
        "print(f\"Test accuracy: {test_acc*100:.1f}\")"
      ],
      "metadata": {
        "id": "c2ZVFP0e1UkB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f78860-a7de-47c3-cc57-bcf2d4a69607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 85.0, AP: 83.3\n",
            "Test accuracy: 45.7\n"
          ]
        }
      ]
    }
  ]
}